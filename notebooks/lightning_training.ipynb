{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import obf functionality\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"../../OBF\").resolve()))\n",
    "\n",
    "from obf.model import ae\n",
    "from obf.model import creator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eyemind.dataloading.load_dataset import limit_sequence_len, get_label_mapper, get_filenames_for_dataset, create_filename_col, get_stratified_group_splits\n",
    "from eyemind.dataloading.gaze_data import GazeDataModule\n",
    "from eyemind.models.classifier import EncoderClassifierModel\n",
    "# from eyemind.models import creator\n",
    "# from eyemind.models import ae \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "from pytorch_lightning import Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"/Users/rickgentry/emotive_lab/eyemind/data/preprocessed/output\")\n",
    "label_filepath = Path(\"/Users/rickgentry/emotive_lab/eyemind/data/EML1_pageLevel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labels and create id\n",
    "label_df = pd.read_csv(label_filepath)\n",
    "label_df = create_filename_col(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParticipantID</th>\n",
       "      <th>Text</th>\n",
       "      <th>PageNum</th>\n",
       "      <th>datetime</th>\n",
       "      <th>unix_start</th>\n",
       "      <th>unix_end</th>\n",
       "      <th>readtime</th>\n",
       "      <th>MW</th>\n",
       "      <th>SVT</th>\n",
       "      <th>Rote_X</th>\n",
       "      <th>Inference_X</th>\n",
       "      <th>Deep_X</th>\n",
       "      <th>Rote_Y</th>\n",
       "      <th>Inference_Y</th>\n",
       "      <th>Rote_Z</th>\n",
       "      <th>Inference_Z</th>\n",
       "      <th>Deep_Z</th>\n",
       "      <th>Rote_D</th>\n",
       "      <th>Inference_D</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EML1_001</td>\n",
       "      <td>Bias</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_001-Bias2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EML1_001</td>\n",
       "      <td>Bias</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_001-Bias6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EML1_001</td>\n",
       "      <td>CausalClaims</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_001-CausalClaims2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EML1_001</td>\n",
       "      <td>CausalClaims</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_001-CausalClaims8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EML1_001</td>\n",
       "      <td>Hypotheses</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_001-Hypotheses1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>EML1_167</td>\n",
       "      <td>Hypotheses</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_167-Hypotheses8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>EML1_167</td>\n",
       "      <td>Validity</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_167-Validity1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>EML1_167</td>\n",
       "      <td>Validity</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_167-Validity7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7869</th>\n",
       "      <td>EML1_167</td>\n",
       "      <td>Variables</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_167-Variables5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7871</th>\n",
       "      <td>EML1_167</td>\n",
       "      <td>Variables</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EML1_167-Variables8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1636 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ParticipantID          Text  PageNum datetime  unix_start  unix_end  \\\n",
       "2         EML1_001          Bias        3      NaN         NaN       NaN   \n",
       "6         EML1_001          Bias        7      NaN         NaN       NaN   \n",
       "12        EML1_001  CausalClaims        3      NaN         NaN       NaN   \n",
       "18        EML1_001  CausalClaims        9      NaN         NaN       NaN   \n",
       "20        EML1_001    Hypotheses        2      NaN         NaN       NaN   \n",
       "...            ...           ...      ...      ...         ...       ...   \n",
       "7859      EML1_167    Hypotheses        9      NaN         NaN       NaN   \n",
       "7860      EML1_167      Validity        2      NaN         NaN       NaN   \n",
       "7864      EML1_167      Validity        8      NaN         NaN       NaN   \n",
       "7869      EML1_167     Variables        6      NaN         NaN       NaN   \n",
       "7871      EML1_167     Variables        9      NaN         NaN       NaN   \n",
       "\n",
       "      readtime   MW  SVT  Rote_X  Inference_X  Deep_X  Rote_Y  Inference_Y  \\\n",
       "2       33.862  1.0  1.0     1.0          1.0     1.0     NaN          NaN   \n",
       "6       23.788  0.0  1.0     1.0          1.0     1.0     NaN          NaN   \n",
       "12      26.138  1.0  0.0     1.0          1.0     1.0     NaN          NaN   \n",
       "18      17.016  1.0  0.0     0.0          1.0     1.0     NaN          NaN   \n",
       "20         NaN  1.0  1.0     1.0          1.0     1.0     NaN          NaN   \n",
       "...        ...  ...  ...     ...          ...     ...     ...          ...   \n",
       "7859       NaN  1.0  1.0     0.0          0.0     0.0     NaN          NaN   \n",
       "7860       NaN  1.0  1.0     1.0          1.0     0.0     NaN          NaN   \n",
       "7864       NaN  1.0  0.0     0.0          1.0     0.0     NaN          NaN   \n",
       "7869       NaN  1.0  1.0     0.0          0.0     1.0     NaN          NaN   \n",
       "7871       NaN  1.0  0.0     0.0          0.0     1.0     NaN          NaN   \n",
       "\n",
       "      Rote_Z  Inference_Z  Deep_Z  Rote_D  Inference_D                filename  \n",
       "2        NaN          NaN     NaN     NaN          NaN          EML1_001-Bias2  \n",
       "6        NaN          NaN     NaN     1.0          NaN          EML1_001-Bias6  \n",
       "12       NaN          NaN     NaN     NaN          NaN  EML1_001-CausalClaims2  \n",
       "18       NaN          NaN     NaN     NaN          NaN  EML1_001-CausalClaims8  \n",
       "20       NaN          NaN     NaN     NaN          NaN    EML1_001-Hypotheses1  \n",
       "...      ...          ...     ...     ...          ...                     ...  \n",
       "7859     NaN          NaN     NaN     NaN          NaN    EML1_167-Hypotheses8  \n",
       "7860     NaN          NaN     NaN     NaN          NaN      EML1_167-Validity1  \n",
       "7864     NaN          NaN     NaN     NaN          NaN      EML1_167-Validity7  \n",
       "7869     NaN          NaN     NaN     NaN          NaN     EML1_167-Variables5  \n",
       "7871     NaN          NaN     NaN     NaN          NaN     EML1_167-Variables8  \n",
       "\n",
       "[1636 rows x 20 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df[~label_df[\"Inference_X\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label columns for creating datasets\n",
    "label_cols = [\"Rote_X\", \"Inference_X\", \"Deep_X\", \"Rote_Y\", \"Inference_Y\", \"Rote_Z\", \"Inference_Z\", \"Deep_Z\", \"Rote_D\", \"Inference_D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(label_cols, label_df, data_folder, x_transforms=None, y_transforms=None, id_col=\"filename\"):\n",
    "    l_ds = []\n",
    "    for label_col in label_cols:\n",
    "        filenames = get_filenames_for_dataset(label_df, data_folder, label_col)\n",
    "        label_mapper = get_label_mapper(label_df, label_col)\n",
    "        ds = GazeDataModule(data_folder, file_list=filenames, label_mapper=label_mapper, transform_x=x_transforms, transform_y=y_transforms)\n",
    "        l_ds.append((label_col,ds))\n",
    "    return l_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ds = get_datasets([\"Rote_X\"], label_df, data_folder, x_transforms=[limit_sequence_len,lambda data: torch.tensor(data).float()], y_transforms=[lambda data: torch.tensor(data).float()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_splits(dms, label_df, id_col=\"filename\"):\n",
    "    all_splits = []\n",
    "    for label_name, dm in dms:\n",
    "        files = [f.split(\".\")[0] for f in dm.file_list]\n",
    "        splits = get_stratified_group_splits(files, label_df, label_name, id_col)\n",
    "        all_splits.append(splits)\n",
    "    return all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f.split(\".\")[0] for f in l_ds[0][1].file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EML1_115-Hypotheses5',\n",
       " 'EML1_125-Hypotheses8',\n",
       " 'EML1_103-Hypotheses4',\n",
       " 'EML1_021-Variables8',\n",
       " 'EML1_041-Variables3',\n",
       " 'EML1_049-CausalClaims6',\n",
       " 'EML1_112-Validity6',\n",
       " 'EML1_096-Validity8',\n",
       " 'EML1_035-Variables8',\n",
       " 'EML1_079-Bias2',\n",
       " 'EML1_078-CausalClaims6',\n",
       " 'EML1_090-Variables8',\n",
       " 'EML1_122-Hypotheses4',\n",
       " 'EML1_066-Hypotheses4',\n",
       " 'EML1_008-Variables8',\n",
       " 'EML1_132-CausalClaims6',\n",
       " 'EML1_029-Bias2',\n",
       " 'EML1_063-Hypotheses8',\n",
       " 'EML1_070-Variables1',\n",
       " 'EML1_048-Hypotheses1',\n",
       " 'EML1_149-Hypotheses4',\n",
       " 'EML1_124-Variables5',\n",
       " 'EML1_121-Hypotheses5',\n",
       " 'EML1_049-Bias6',\n",
       " 'EML1_133-Bias6',\n",
       " 'EML1_068-Bias2',\n",
       " 'EML1_041-Hypotheses1',\n",
       " 'EML1_035-Hypotheses5',\n",
       " 'EML1_003-Bias2',\n",
       " 'EML1_047-Variables5',\n",
       " 'EML1_056-Hypotheses5',\n",
       " 'EML1_014-Variables3',\n",
       " 'EML1_116-Variables3',\n",
       " 'EML1_063-CausalClaims3',\n",
       " 'EML1_012-CausalClaims8',\n",
       " 'EML1_041-Bias8',\n",
       " 'EML1_049-Bias8',\n",
       " 'EML1_134-Variables8',\n",
       " 'EML1_025-Variables3',\n",
       " 'EML1_102-Variables1',\n",
       " 'EML1_026-Validity8',\n",
       " 'EML1_101-CausalClaims6',\n",
       " 'EML1_049-Variables5',\n",
       " 'EML1_045-Variables1',\n",
       " 'EML1_033-Hypotheses1',\n",
       " 'EML1_136-Validity6',\n",
       " 'EML1_082-Bias6',\n",
       " 'EML1_028-CausalClaims6',\n",
       " 'EML1_040-Hypotheses5',\n",
       " 'EML1_031-Validity8',\n",
       " 'EML1_074-Hypotheses1',\n",
       " 'EML1_100-CausalClaims8',\n",
       " 'EML1_058-Validity6',\n",
       " 'EML1_007-CausalClaims2',\n",
       " 'EML1_055-Validity1',\n",
       " 'EML1_036-Validity7',\n",
       " 'EML1_126-Validity6',\n",
       " 'EML1_080-Hypotheses4',\n",
       " 'EML1_103-Bias6',\n",
       " 'EML1_024-Variables1',\n",
       " 'EML1_025-CausalClaims2',\n",
       " 'EML1_037-Hypotheses1',\n",
       " 'EML1_007-Validity1',\n",
       " 'EML1_118-CausalClaims6',\n",
       " 'EML1_045-CausalClaims8',\n",
       " 'EML1_104-Validity8',\n",
       " 'EML1_080-Bias6',\n",
       " 'EML1_010-Hypotheses4',\n",
       " 'EML1_069-Variables3',\n",
       " 'EML1_080-Variables5',\n",
       " 'EML1_005-Hypotheses4',\n",
       " 'EML1_053-Variables1',\n",
       " 'EML1_044-Variables3',\n",
       " 'EML1_092-Bias2',\n",
       " 'EML1_094-Hypotheses1',\n",
       " 'EML1_024-Validity6',\n",
       " 'EML1_111-Variables1',\n",
       " 'EML1_072-Validity8',\n",
       " 'EML1_129-Hypotheses8',\n",
       " 'EML1_111-CausalClaims6',\n",
       " 'EML1_060-Validity8',\n",
       " 'EML1_048-Variables3',\n",
       " 'EML1_028-Variables8',\n",
       " 'EML1_134-Hypotheses1',\n",
       " 'EML1_116-Hypotheses8',\n",
       " 'EML1_029-CausalClaims6',\n",
       " 'EML1_134-Validity8',\n",
       " 'EML1_012-Bias2',\n",
       " 'EML1_124-Hypotheses4',\n",
       " 'EML1_128-Hypotheses5',\n",
       " 'EML1_099-Validity6',\n",
       " 'EML1_108-Bias6',\n",
       " 'EML1_063-Validity8',\n",
       " 'EML1_036-Bias6',\n",
       " 'EML1_046-CausalClaims3',\n",
       " 'EML1_031-Validity1',\n",
       " 'EML1_067-Hypotheses1',\n",
       " 'EML1_109-Hypotheses4',\n",
       " 'EML1_121-Bias7',\n",
       " 'EML1_094-Hypotheses8',\n",
       " 'EML1_081-CausalClaims3',\n",
       " 'EML1_082-Bias8',\n",
       " 'EML1_025-Bias2',\n",
       " 'EML1_045-Validity6',\n",
       " 'EML1_043-Hypotheses5',\n",
       " 'EML1_031-Variables8',\n",
       " 'EML1_096-CausalClaims6',\n",
       " 'EML1_107-Hypotheses1',\n",
       " 'EML1_058-Variables5',\n",
       " 'EML1_114-Variables5',\n",
       " 'EML1_134-CausalClaims8',\n",
       " 'EML1_089-Validity6',\n",
       " 'EML1_127-Variables3',\n",
       " 'EML1_110-Hypotheses1',\n",
       " 'EML1_051-Bias6',\n",
       " 'EML1_098-Validity1',\n",
       " 'EML1_038-Hypotheses1',\n",
       " 'EML1_067-Bias8',\n",
       " 'EML1_056-Variables3',\n",
       " 'EML1_120-Hypotheses8',\n",
       " 'EML1_099-Bias2',\n",
       " 'EML1_135-Validity6',\n",
       " 'EML1_083-Bias2',\n",
       " 'EML1_010-Variables5',\n",
       " 'EML1_100-Validity1',\n",
       " 'EML1_030-Validity1',\n",
       " 'EML1_133-Hypotheses8',\n",
       " 'EML1_035-Bias6',\n",
       " 'EML1_053-Validity8',\n",
       " 'EML1_157-Hypotheses8',\n",
       " 'EML1_039-Bias2',\n",
       " 'EML1_024-CausalClaims6',\n",
       " 'EML1_014-CausalClaims6',\n",
       " 'EML1_126-Bias8',\n",
       " 'EML1_065-Hypotheses5',\n",
       " 'EML1_065-CausalClaims6',\n",
       " 'EML1_041-Hypotheses8',\n",
       " 'EML1_112-Variables5',\n",
       " 'EML1_075-Bias6',\n",
       " 'EML1_097-Hypotheses8',\n",
       " 'EML1_030-Variables5',\n",
       " 'EML1_097-Validity6',\n",
       " 'EML1_124-Hypotheses8',\n",
       " 'EML1_091-Hypotheses8',\n",
       " 'EML1_008-Variables5',\n",
       " 'EML1_112-CausalClaims6',\n",
       " 'EML1_036-Bias8',\n",
       " 'EML1_146-CausalClaims8',\n",
       " 'EML1_129-Validity8',\n",
       " 'EML1_062-CausalClaims6',\n",
       " 'EML1_016-Hypotheses8',\n",
       " 'EML1_095-CausalClaims2',\n",
       " 'EML1_100-Variables3',\n",
       " 'EML1_007-Variables3',\n",
       " 'EML1_036-Hypotheses8',\n",
       " 'EML1_018-Validity1',\n",
       " 'EML1_055-Bias2',\n",
       " 'EML1_017-Hypotheses5',\n",
       " 'EML1_038-Bias2',\n",
       " 'EML1_050-CausalClaims8',\n",
       " 'EML1_084-Variables3',\n",
       " 'EML1_102-Validity6',\n",
       " 'EML1_123-Validity8',\n",
       " 'EML1_008-Hypotheses4',\n",
       " 'EML1_063-Hypotheses5',\n",
       " 'EML1_072-Variables8',\n",
       " 'EML1_143-Variables8',\n",
       " 'EML1_133-Bias8',\n",
       " 'EML1_055-Hypotheses4',\n",
       " 'EML1_016-CausalClaims8',\n",
       " 'EML1_096-Hypotheses8',\n",
       " 'EML1_124-Validity7',\n",
       " 'EML1_030-Variables3',\n",
       " 'EML1_029-Hypotheses1',\n",
       " 'EML1_084-CausalClaims8',\n",
       " 'EML1_038-Validity1',\n",
       " 'EML1_089-CausalClaims6',\n",
       " 'EML1_078-CausalClaims2',\n",
       " 'EML1_060-Bias2',\n",
       " 'EML1_047-Hypotheses8',\n",
       " 'EML1_092-Variables5',\n",
       " 'EML1_024-Validity8',\n",
       " 'EML1_020-Validity8',\n",
       " 'EML1_043-Hypotheses8',\n",
       " 'EML1_124-CausalClaims3',\n",
       " 'EML1_135-Hypotheses1',\n",
       " 'EML1_122-Bias8',\n",
       " 'EML1_018-Bias2',\n",
       " 'EML1_135-CausalClaims6',\n",
       " 'EML1_039-CausalClaims2',\n",
       " 'EML1_035-Validity6',\n",
       " 'EML1_019-Bias6',\n",
       " 'EML1_033-Variables5',\n",
       " 'EML1_126-Validity8',\n",
       " 'EML1_050-CausalClaims3',\n",
       " 'EML1_060-Variables5',\n",
       " 'EML1_031-Bias2',\n",
       " 'EML1_031-CausalClaims8',\n",
       " 'EML1_022-CausalClaims2',\n",
       " 'EML1_098-CausalClaims6',\n",
       " 'EML1_112-Hypotheses5',\n",
       " 'EML1_075-Variables5',\n",
       " 'EML1_115-Bias8',\n",
       " 'EML1_038-Variables1',\n",
       " 'EML1_012-Variables8',\n",
       " 'EML1_093-Validity1',\n",
       " 'EML1_152-Bias8',\n",
       " 'EML1_051-Variables3',\n",
       " 'EML1_059-Variables3',\n",
       " 'EML1_102-Variables8',\n",
       " 'EML1_018-Validity8',\n",
       " 'EML1_136-Hypotheses4',\n",
       " 'EML1_050-Bias2',\n",
       " 'EML1_006-Hypotheses4',\n",
       " 'EML1_121-Variables1',\n",
       " 'EML1_024-Variables8',\n",
       " 'EML1_016-Validity6',\n",
       " 'EML1_119-Variables5',\n",
       " 'EML1_010-Validity7',\n",
       " 'EML1_021-Variables1',\n",
       " 'EML1_011-Variables8',\n",
       " 'EML1_111-Bias8',\n",
       " 'EML1_038-CausalClaims6',\n",
       " 'EML1_032-Validity6',\n",
       " 'EML1_051-Bias8',\n",
       " 'EML1_081-Hypotheses4',\n",
       " 'EML1_129-CausalClaims3',\n",
       " 'EML1_109-CausalClaims2',\n",
       " 'EML1_017-Validity8',\n",
       " 'EML1_126-CausalClaims6',\n",
       " 'EML1_037-CausalClaims3',\n",
       " 'EML1_097-Hypotheses4',\n",
       " 'EML1_063-Validity6',\n",
       " 'EML1_033-CausalClaims2',\n",
       " 'EML1_127-Hypotheses4',\n",
       " 'EML1_019-Variables8',\n",
       " 'EML1_041-CausalClaims8',\n",
       " 'EML1_101-Validity1',\n",
       " 'EML1_003-Hypotheses4',\n",
       " 'EML1_105-Hypotheses4',\n",
       " 'EML1_115-Validity6',\n",
       " 'EML1_120-Variables3',\n",
       " 'EML1_103-Hypotheses1',\n",
       " 'EML1_133-Validity6',\n",
       " 'EML1_087-CausalClaims2',\n",
       " 'EML1_093-Hypotheses8',\n",
       " 'EML1_053-CausalClaims3',\n",
       " 'EML1_098-Hypotheses8',\n",
       " 'EML1_110-CausalClaims2',\n",
       " 'EML1_085-Bias2',\n",
       " 'EML1_126-Bias6',\n",
       " 'EML1_117-Bias2',\n",
       " 'EML1_027-CausalClaims8',\n",
       " 'EML1_048-Hypotheses4',\n",
       " 'EML1_110-Hypotheses5',\n",
       " 'EML1_119-CausalClaims2',\n",
       " 'EML1_040-Validity1',\n",
       " 'EML1_074-Bias6',\n",
       " 'EML1_081-Bias2',\n",
       " 'EML1_005-Validity7',\n",
       " 'EML1_029-Validity8',\n",
       " 'EML1_044-Validity6',\n",
       " 'EML1_108-CausalClaims3',\n",
       " 'EML1_004-Validity1',\n",
       " 'EML1_075-Validity1',\n",
       " 'EML1_052-Variables3',\n",
       " 'EML1_014-Hypotheses1',\n",
       " 'EML1_062-Hypotheses8',\n",
       " 'EML1_016-Bias2',\n",
       " 'EML1_067-Validity8',\n",
       " 'EML1_019-Hypotheses8',\n",
       " 'EML1_089-Variables8',\n",
       " 'EML1_076-Hypotheses8',\n",
       " 'EML1_076-Bias6',\n",
       " 'EML1_039-CausalClaims6',\n",
       " 'EML1_097-CausalClaims2',\n",
       " 'EML1_150-Variables8',\n",
       " 'EML1_086-CausalClaims8',\n",
       " 'EML1_050-Hypotheses1',\n",
       " 'EML1_083-Variables3',\n",
       " 'EML1_058-Hypotheses1',\n",
       " 'EML1_074-Variables3',\n",
       " 'EML1_007-Validity6',\n",
       " 'EML1_117-Validity6',\n",
       " 'EML1_107-CausalClaims6',\n",
       " 'EML1_026-CausalClaims6',\n",
       " 'EML1_117-Variables8',\n",
       " 'EML1_107-Hypotheses8',\n",
       " 'EML1_027-Bias8',\n",
       " 'EML1_102-Hypotheses1',\n",
       " 'EML1_110-Bias2',\n",
       " 'EML1_107-Bias2',\n",
       " 'EML1_083-Bias8',\n",
       " 'EML1_069-Bias8',\n",
       " 'EML1_111-Validity6',\n",
       " 'EML1_097-Variables1',\n",
       " 'EML1_094-Bias2',\n",
       " 'EML1_086-Variables5',\n",
       " 'EML1_121-Validity6',\n",
       " 'EML1_098-CausalClaims8',\n",
       " 'EML1_044-Variables5',\n",
       " 'EML1_030-CausalClaims3',\n",
       " 'EML1_125-Hypotheses4',\n",
       " 'EML1_062-Bias2',\n",
       " 'EML1_008-Hypotheses1',\n",
       " 'EML1_017-Variables1',\n",
       " 'EML1_033-CausalClaims6',\n",
       " 'EML1_052-CausalClaims2',\n",
       " 'EML1_130-Validity1',\n",
       " 'EML1_017-Variables8',\n",
       " 'EML1_108-Hypotheses1',\n",
       " 'EML1_099-Variables3',\n",
       " 'EML1_130-Variables1',\n",
       " 'EML1_098-Hypotheses5',\n",
       " 'EML1_020-Bias8',\n",
       " 'EML1_003-Validity6',\n",
       " 'EML1_132-Variables8',\n",
       " 'EML1_004-Variables3',\n",
       " 'EML1_120-Bias6',\n",
       " 'EML1_096-Variables5',\n",
       " 'EML1_094-Variables1',\n",
       " 'EML1_075-Hypotheses8',\n",
       " 'EML1_082-Hypotheses1',\n",
       " 'EML1_105-Variables5',\n",
       " 'EML1_022-Hypotheses4',\n",
       " 'EML1_025-CausalClaims6',\n",
       " 'EML1_019-CausalClaims3',\n",
       " 'EML1_052-CausalClaims6',\n",
       " 'EML1_100-Bias6',\n",
       " 'EML1_097-Validity1',\n",
       " 'EML1_068-Validity7',\n",
       " 'EML1_122-CausalClaims8',\n",
       " 'EML1_113-Validity8',\n",
       " 'EML1_079-CausalClaims6',\n",
       " 'EML1_108-Validity1',\n",
       " 'EML1_125-Variables3',\n",
       " 'EML1_070-CausalClaims3',\n",
       " 'EML1_050-Validity1',\n",
       " 'EML1_072-Hypotheses1',\n",
       " 'EML1_086-Validity6',\n",
       " 'EML1_038-Bias8',\n",
       " 'EML1_007-Hypotheses5',\n",
       " 'EML1_096-CausalClaims8',\n",
       " 'EML1_026-Bias7',\n",
       " 'EML1_032-Hypotheses8',\n",
       " 'EML1_017-CausalClaims3',\n",
       " 'EML1_037-Hypotheses8',\n",
       " 'EML1_026-Hypotheses4',\n",
       " 'EML1_112-Hypotheses1',\n",
       " 'EML1_082-CausalClaims6',\n",
       " 'EML1_015-Variables5',\n",
       " 'EML1_028-CausalClaims3',\n",
       " 'EML1_011-Bias6',\n",
       " 'EML1_099-Hypotheses1',\n",
       " 'EML1_011-Validity1',\n",
       " 'EML1_012-Hypotheses8',\n",
       " 'EML1_139-CausalClaims8',\n",
       " 'EML1_106-Variables8',\n",
       " 'EML1_101-CausalClaims8',\n",
       " 'EML1_120-Hypotheses5',\n",
       " 'EML1_114-Hypotheses1',\n",
       " 'EML1_053-Bias8',\n",
       " 'EML1_030-Validity7',\n",
       " 'EML1_076-CausalClaims6',\n",
       " 'EML1_117-Variables1',\n",
       " 'EML1_043-Bias7',\n",
       " 'EML1_029-Validity1',\n",
       " 'EML1_021-Hypotheses5',\n",
       " 'EML1_062-Hypotheses5',\n",
       " 'EML1_025-Validity1',\n",
       " 'EML1_116-Validity1',\n",
       " 'EML1_136-CausalClaims8',\n",
       " 'EML1_016-Variables1',\n",
       " 'EML1_021-Bias2',\n",
       " 'EML1_114-Validity6',\n",
       " 'EML1_106-Hypotheses4',\n",
       " 'EML1_016-CausalClaims2',\n",
       " 'EML1_053-Bias2',\n",
       " 'EML1_076-Validity8',\n",
       " 'EML1_087-Variables1',\n",
       " 'EML1_064-Validity1',\n",
       " 'EML1_105-Validity7',\n",
       " 'EML1_082-Variables5',\n",
       " 'EML1_108-CausalClaims8',\n",
       " 'EML1_047-Bias7',\n",
       " 'EML1_065-Bias6',\n",
       " 'EML1_085-CausalClaims6',\n",
       " 'EML1_128-Variables5',\n",
       " 'EML1_070-Bias2',\n",
       " 'EML1_064-Variables8',\n",
       " 'EML1_055-CausalClaims8',\n",
       " 'EML1_145-CausalClaims8',\n",
       " 'EML1_083-Validity8',\n",
       " 'EML1_018-Hypotheses8',\n",
       " 'EML1_117-Hypotheses4',\n",
       " 'EML1_015-Validity6',\n",
       " 'EML1_058-Validity8',\n",
       " 'EML1_046-Variables8',\n",
       " 'EML1_025-Bias7',\n",
       " 'EML1_031-Hypotheses8',\n",
       " 'EML1_071-Validity6',\n",
       " 'EML1_075-Bias8',\n",
       " 'EML1_099-Hypotheses8',\n",
       " 'EML1_055-Variables1',\n",
       " 'EML1_096-Bias2',\n",
       " 'EML1_122-Hypotheses1',\n",
       " 'EML1_068-Variables1',\n",
       " 'EML1_133-CausalClaims3',\n",
       " 'EML1_022-Variables1',\n",
       " 'EML1_079-Validity6',\n",
       " 'EML1_098-Bias2',\n",
       " 'EML1_075-CausalClaims8',\n",
       " 'EML1_042-Bias2',\n",
       " 'EML1_106-Variables1',\n",
       " 'EML1_011-Variables1',\n",
       " 'EML1_044-CausalClaims3',\n",
       " 'EML1_104-Bias8',\n",
       " 'EML1_090-Bias2',\n",
       " 'EML1_017-Bias8',\n",
       " 'EML1_118-Bias8',\n",
       " 'EML1_021-Bias6',\n",
       " 'EML1_091-Bias8',\n",
       " 'EML1_032-CausalClaims3',\n",
       " 'EML1_019-Validity7',\n",
       " 'EML1_045-Validity8',\n",
       " 'EML1_051-Validity1',\n",
       " 'EML1_109-Variables1',\n",
       " 'EML1_050-Variables8',\n",
       " 'EML1_119-Bias2',\n",
       " 'EML1_125-CausalClaims6',\n",
       " 'EML1_030-CausalClaims6',\n",
       " 'EML1_063-Bias8',\n",
       " 'EML1_080-Validity8',\n",
       " 'EML1_071-Bias2',\n",
       " 'EML1_031-CausalClaims6',\n",
       " 'EML1_071-Hypotheses8',\n",
       " 'EML1_060-Validity6',\n",
       " 'EML1_095-Variables3',\n",
       " 'EML1_036-Variables5',\n",
       " 'EML1_130-Hypotheses8',\n",
       " 'EML1_071-Validity8',\n",
       " 'EML1_120-Variables5',\n",
       " 'EML1_018-CausalClaims3',\n",
       " 'EML1_116-CausalClaims3',\n",
       " 'EML1_035-Validity8',\n",
       " 'EML1_063-CausalClaims6',\n",
       " 'EML1_087-Bias2',\n",
       " 'EML1_116-Hypotheses5',\n",
       " 'EML1_127-Variables5',\n",
       " 'EML1_053-CausalClaims6',\n",
       " 'EML1_114-Bias2',\n",
       " 'EML1_083-CausalClaims6',\n",
       " 'EML1_038-CausalClaims2',\n",
       " 'EML1_091-Validity6',\n",
       " 'EML1_102-Bias7',\n",
       " 'EML1_110-Variables3',\n",
       " 'EML1_047-Validity8',\n",
       " 'EML1_053-Variables5',\n",
       " 'EML1_089-Hypotheses5',\n",
       " 'EML1_091-Validity8',\n",
       " 'EML1_011-Bias8',\n",
       " 'EML1_104-Variables1',\n",
       " 'EML1_118-Hypotheses1',\n",
       " 'EML1_114-CausalClaims6',\n",
       " 'EML1_122-Variables5',\n",
       " 'EML1_053-Hypotheses1',\n",
       " 'EML1_047-Variables3',\n",
       " 'EML1_102-CausalClaims8',\n",
       " 'EML1_092-Bias8',\n",
       " 'EML1_027-Validity1',\n",
       " 'EML1_123-Validity6',\n",
       " 'EML1_090-Hypotheses5',\n",
       " 'EML1_127-CausalClaims3',\n",
       " 'EML1_006-Validity1',\n",
       " 'EML1_045-Hypotheses1',\n",
       " 'EML1_130-Bias8',\n",
       " 'EML1_106-CausalClaims3',\n",
       " 'EML1_059-Validity1',\n",
       " 'EML1_096-Bias6',\n",
       " 'EML1_109-CausalClaims6',\n",
       " 'EML1_068-Hypotheses1',\n",
       " 'EML1_112-CausalClaims3',\n",
       " 'EML1_085-Hypotheses1',\n",
       " 'EML1_102-Hypotheses8',\n",
       " 'EML1_011-CausalClaims8',\n",
       " 'EML1_010-Bias2',\n",
       " 'EML1_030-Hypotheses4',\n",
       " 'EML1_091-Variables1',\n",
       " 'EML1_031-Hypotheses4',\n",
       " 'EML1_113-Hypotheses5',\n",
       " 'EML1_030-Bias8',\n",
       " 'EML1_121-Hypotheses8',\n",
       " 'EML1_005-Variables3',\n",
       " 'EML1_014-Hypotheses5',\n",
       " 'EML1_058-Bias6',\n",
       " 'EML1_051-CausalClaims6',\n",
       " 'EML1_059-Hypotheses8',\n",
       " 'EML1_153-Variables3',\n",
       " 'EML1_081-Hypotheses1',\n",
       " 'EML1_068-Validity1',\n",
       " 'EML1_078-Variables1',\n",
       " 'EML1_122-Bias2',\n",
       " 'EML1_034-Bias8',\n",
       " 'EML1_004-Bias8',\n",
       " 'EML1_091-Hypotheses4',\n",
       " 'EML1_125-Validity8',\n",
       " 'EML1_113-Bias2',\n",
       " 'EML1_027-Validity7',\n",
       " 'EML1_072-Bias6',\n",
       " 'EML1_095-Bias7',\n",
       " 'EML1_010-Hypotheses8',\n",
       " 'EML1_016-Validity1',\n",
       " 'EML1_022-Hypotheses1',\n",
       " 'EML1_123-Bias7',\n",
       " 'EML1_069-Bias6',\n",
       " 'EML1_040-Variables3',\n",
       " 'EML1_069-CausalClaims3',\n",
       " 'EML1_130-Variables8',\n",
       " 'EML1_071-Bias8',\n",
       " 'EML1_076-Variables8',\n",
       " 'EML1_086-Validity1',\n",
       " 'EML1_065-Validity6',\n",
       " 'EML1_127-Bias8',\n",
       " 'EML1_128-CausalClaims8',\n",
       " 'EML1_144-Bias8',\n",
       " 'EML1_060-Hypotheses1',\n",
       " 'EML1_089-Bias2',\n",
       " 'EML1_041-Variables1',\n",
       " 'EML1_055-CausalClaims2',\n",
       " 'EML1_043-Variables3',\n",
       " 'EML1_095-Bias2',\n",
       " 'EML1_015-Bias2',\n",
       " 'EML1_038-Variables8',\n",
       " 'EML1_053-Validity6',\n",
       " 'EML1_024-Bias6',\n",
       " 'EML1_072-Hypotheses5',\n",
       " 'EML1_103-CausalClaims3',\n",
       " 'EML1_083-Hypotheses8',\n",
       " 'EML1_015-Validity8',\n",
       " 'EML1_087-Bias8',\n",
       " 'EML1_010-CausalClaims8',\n",
       " 'EML1_095-Validity6',\n",
       " 'EML1_014-Variables1',\n",
       " 'EML1_014-Bias6',\n",
       " 'EML1_062-Bias6',\n",
       " 'EML1_037-Bias6',\n",
       " 'EML1_124-Bias6',\n",
       " 'EML1_099-Variables8',\n",
       " 'EML1_020-Variables8',\n",
       " 'EML1_112-Variables3',\n",
       " 'EML1_085-Bias8',\n",
       " 'EML1_035-Hypotheses1',\n",
       " 'EML1_078-Variables5',\n",
       " 'EML1_040-CausalClaims6',\n",
       " 'EML1_093-Variables5',\n",
       " 'EML1_018-Variables5',\n",
       " 'EML1_100-Bias8',\n",
       " 'EML1_107-Validity1',\n",
       " 'EML1_133-CausalClaims8',\n",
       " 'EML1_121-Bias2',\n",
       " 'EML1_049-Variables3',\n",
       " 'EML1_033-Variables3',\n",
       " 'EML1_010-CausalClaims6',\n",
       " 'EML1_060-Variables3',\n",
       " 'EML1_047-Bias2',\n",
       " 'EML1_049-CausalClaims2',\n",
       " 'EML1_028-Validity6',\n",
       " 'EML1_068-Hypotheses5',\n",
       " 'EML1_135-CausalClaims3',\n",
       " 'EML1_114-Bias6',\n",
       " 'EML1_100-Hypotheses8',\n",
       " 'EML1_122-Variables1',\n",
       " 'EML1_104-Bias6',\n",
       " 'EML1_123-CausalClaims8',\n",
       " 'EML1_121-CausalClaims2',\n",
       " 'EML1_007-Bias6',\n",
       " 'EML1_071-Hypotheses1',\n",
       " 'EML1_043-CausalClaims8',\n",
       " 'EML1_025-Validity6',\n",
       " 'EML1_032-Hypotheses5',\n",
       " 'EML1_112-Bias2',\n",
       " 'EML1_136-Bias8',\n",
       " 'EML1_131-Variables8',\n",
       " 'EML1_021-CausalClaims8',\n",
       " 'EML1_005-Bias2',\n",
       " 'EML1_062-Validity8',\n",
       " 'EML1_079-Validity8',\n",
       " 'EML1_091-CausalClaims2',\n",
       " 'EML1_079-CausalClaims2',\n",
       " 'EML1_152-Validity8',\n",
       " 'EML1_070-Variables8',\n",
       " 'EML1_107-Validity6',\n",
       " 'EML1_110-Bias8',\n",
       " 'EML1_003-Bias8',\n",
       " 'EML1_030-Bias6',\n",
       " 'EML1_107-Variables8',\n",
       " 'EML1_074-Variables1',\n",
       " 'EML1_062-CausalClaims8',\n",
       " 'EML1_113-Validity6',\n",
       " 'EML1_014-Validity1',\n",
       " 'EML1_116-Validity7',\n",
       " 'EML1_148-Validity8',\n",
       " 'EML1_104-CausalClaims6',\n",
       " 'EML1_042-Variables8',\n",
       " 'EML1_033-Hypotheses4',\n",
       " 'EML1_119-Validity6',\n",
       " 'EML1_090-Bias6',\n",
       " 'EML1_093-CausalClaims8',\n",
       " 'EML1_053-Hypotheses4',\n",
       " 'EML1_079-Hypotheses4',\n",
       " 'EML1_058-Variables3',\n",
       " 'EML1_021-Validity8',\n",
       " 'EML1_035-Bias2',\n",
       " 'EML1_102-Validity8',\n",
       " 'EML1_039-Bias7',\n",
       " 'EML1_056-Validity6',\n",
       " 'EML1_084-Bias6',\n",
       " 'EML1_140-Hypotheses8',\n",
       " 'EML1_114-Validity1',\n",
       " 'EML1_033-Validity1',\n",
       " 'EML1_069-Variables1',\n",
       " 'EML1_115-Bias6',\n",
       " 'EML1_051-Validity8',\n",
       " 'EML1_049-Hypotheses1',\n",
       " 'EML1_021-CausalClaims2',\n",
       " 'EML1_070-Hypotheses1',\n",
       " 'EML1_094-Bias7',\n",
       " 'EML1_083-Validity1',\n",
       " 'EML1_007-Variables8',\n",
       " 'EML1_109-Bias2',\n",
       " 'EML1_018-Hypotheses5',\n",
       " 'EML1_136-CausalClaims3',\n",
       " 'EML1_123-Hypotheses1',\n",
       " 'EML1_087-Variables5',\n",
       " 'EML1_027-CausalClaims2',\n",
       " 'EML1_004-Hypotheses8',\n",
       " 'EML1_128-Bias6',\n",
       " 'EML1_090-Validity7',\n",
       " 'EML1_076-Hypotheses4',\n",
       " 'EML1_095-Hypotheses1',\n",
       " 'EML1_065-Variables8',\n",
       " 'EML1_026-Variables1',\n",
       " 'EML1_046-Hypotheses5',\n",
       " 'EML1_037-Bias8',\n",
       " 'EML1_005-Variables1',\n",
       " 'EML1_090-Variables5',\n",
       " 'EML1_109-Bias8',\n",
       " 'EML1_089-Variables3',\n",
       " 'EML1_135-Validity8',\n",
       " 'EML1_092-CausalClaims6',\n",
       " 'EML1_107-Bias6',\n",
       " 'EML1_066-CausalClaims3',\n",
       " 'EML1_106-Bias7',\n",
       " 'EML1_120-CausalClaims2',\n",
       " 'EML1_086-Variables8',\n",
       " 'EML1_071-CausalClaims2',\n",
       " 'EML1_015-Hypotheses8',\n",
       " 'EML1_097-CausalClaims6',\n",
       " 'EML1_056-Validity8',\n",
       " 'EML1_055-Hypotheses1',\n",
       " 'EML1_087-Hypotheses8',\n",
       " 'EML1_130-Hypotheses1',\n",
       " 'EML1_072-Validity6',\n",
       " 'EML1_096-Validity1',\n",
       " 'EML1_068-CausalClaims3',\n",
       " 'EML1_036-Variables1',\n",
       " 'EML1_064-Bias8',\n",
       " 'EML1_067-CausalClaims6',\n",
       " 'EML1_128-Validity1',\n",
       " 'EML1_064-Variables3',\n",
       " 'EML1_084-CausalClaims2',\n",
       " 'EML1_137-Validity8',\n",
       " 'EML1_105-Validity1',\n",
       " 'EML1_071-Variables8',\n",
       " 'EML1_132-Validity7',\n",
       " 'EML1_082-Hypotheses4',\n",
       " 'EML1_020-CausalClaims6',\n",
       " 'EML1_028-Hypotheses5',\n",
       " 'EML1_121-Validity1',\n",
       " 'EML1_044-Hypotheses5',\n",
       " 'EML1_069-Validity7',\n",
       " 'EML1_064-Hypotheses8',\n",
       " 'EML1_010-Bias8',\n",
       " 'EML1_085-Validity7',\n",
       " 'EML1_084-Hypotheses8',\n",
       " 'EML1_024-Hypotheses1',\n",
       " 'EML1_123-Bias2',\n",
       " 'EML1_092-Validity6',\n",
       " 'EML1_119-CausalClaims6',\n",
       " 'EML1_049-Hypotheses8',\n",
       " 'EML1_127-Hypotheses1',\n",
       " 'EML1_090-CausalClaims6',\n",
       " 'EML1_133-Variables3',\n",
       " 'EML1_059-CausalClaims8',\n",
       " 'EML1_094-Validity8',\n",
       " 'EML1_048-CausalClaims2',\n",
       " 'EML1_037-Variables8',\n",
       " 'EML1_048-Validity1',\n",
       " 'EML1_035-CausalClaims6',\n",
       " 'EML1_048-Validity6',\n",
       " 'EML1_111-Bias2',\n",
       " 'EML1_064-Validity6',\n",
       " 'EML1_016-Variables5',\n",
       " 'EML1_108-Variables5',\n",
       " 'EML1_062-Validity1',\n",
       " 'EML1_136-Hypotheses8',\n",
       " 'EML1_126-CausalClaims3',\n",
       " 'EML1_027-Bias6',\n",
       " 'EML1_102-Bias2',\n",
       " 'EML1_109-Validity1',\n",
       " 'EML1_103-Bias2',\n",
       " 'EML1_119-Validity1',\n",
       " 'EML1_045-Bias8',\n",
       " 'EML1_084-Validity1',\n",
       " 'EML1_154-CausalClaims6',\n",
       " 'EML1_026-CausalClaims2',\n",
       " 'EML1_068-Bias7',\n",
       " 'EML1_006-Hypotheses1',\n",
       " 'EML1_062-Variables8',\n",
       " 'EML1_015-Bias7',\n",
       " 'EML1_074-Hypotheses4',\n",
       " 'EML1_080-CausalClaims8',\n",
       " 'EML1_095-CausalClaims6',\n",
       " 'EML1_044-Bias6',\n",
       " 'EML1_019-Variables5',\n",
       " 'EML1_072-Bias2',\n",
       " 'EML1_015-Hypotheses1',\n",
       " 'EML1_032-Validity8',\n",
       " 'EML1_109-Validity7',\n",
       " 'EML1_093-Variables8',\n",
       " 'EML1_024-Bias8',\n",
       " 'EML1_092-Validity8',\n",
       " 'EML1_027-Variables5',\n",
       " 'EML1_078-Bias8',\n",
       " 'EML1_040-Bias6',\n",
       " 'EML1_116-Bias6',\n",
       " 'EML1_044-Hypotheses1',\n",
       " 'EML1_059-Validity8',\n",
       " 'EML1_040-Variables5',\n",
       " 'EML1_130-CausalClaims2',\n",
       " 'EML1_028-Bias8',\n",
       " 'EML1_102-CausalClaims3',\n",
       " 'EML1_098-Validity7',\n",
       " 'EML1_134-CausalClaims2',\n",
       " 'EML1_157-Validity6',\n",
       " 'EML1_066-Bias2',\n",
       " 'EML1_121-Variables3',\n",
       " 'EML1_082-Variables3',\n",
       " 'EML1_128-Variables8',\n",
       " 'EML1_012-Validity1',\n",
       " 'EML1_118-Hypotheses5',\n",
       " 'EML1_107-Variables1',\n",
       " 'EML1_026-Bias2',\n",
       " 'EML1_106-Hypotheses1',\n",
       " 'EML1_128-CausalClaims6',\n",
       " 'EML1_109-Variables3',\n",
       " 'EML1_094-CausalClaims2',\n",
       " 'EML1_095-Validity1',\n",
       " 'EML1_091-Bias6',\n",
       " 'EML1_132-Bias2',\n",
       " 'EML1_015-Variables8',\n",
       " 'EML1_065-Hypotheses8',\n",
       " 'EML1_042-Hypotheses1',\n",
       " 'EML1_126-Variables5',\n",
       " 'EML1_096-Variables8',\n",
       " 'EML1_005-Validity1',\n",
       " 'EML1_018-CausalClaims8',\n",
       " 'EML1_104-Validity1',\n",
       " 'EML1_118-Validity8',\n",
       " 'EML1_058-CausalClaims6',\n",
       " 'EML1_006-Variables5',\n",
       " 'EML1_043-Validity8',\n",
       " 'EML1_084-Variables8',\n",
       " 'EML1_014-Validity6',\n",
       " 'EML1_099-CausalClaims6',\n",
       " 'EML1_078-Bias2',\n",
       " 'EML1_068-CausalClaims8',\n",
       " 'EML1_019-Hypotheses5',\n",
       " 'EML1_021-Validity6',\n",
       " 'EML1_046-Hypotheses1',\n",
       " 'EML1_014-CausalClaims3',\n",
       " 'EML1_127-Validity8',\n",
       " 'EML1_087-CausalClaims6',\n",
       " 'EML1_040-Hypotheses1',\n",
       " 'EML1_101-Bias8',\n",
       " 'EML1_124-Variables8',\n",
       " 'EML1_076-Validity1',\n",
       " 'EML1_008-CausalClaims6',\n",
       " 'EML1_050-Validity7',\n",
       " 'EML1_006-Variables1',\n",
       " 'EML1_075-Validity6',\n",
       " 'EML1_022-Bias8',\n",
       " 'EML1_049-Validity6',\n",
       " 'EML1_020-CausalClaims2',\n",
       " 'EML1_043-CausalClaims2',\n",
       " 'EML1_116-Bias2',\n",
       " 'EML1_020-Hypotheses1',\n",
       " 'EML1_095-Variables5',\n",
       " 'EML1_054-Hypotheses8',\n",
       " 'EML1_039-Validity8',\n",
       " 'EML1_056-Bias7',\n",
       " 'EML1_079-Hypotheses1',\n",
       " 'EML1_127-CausalClaims6',\n",
       " 'EML1_086-Hypotheses8',\n",
       " 'EML1_155-Bias7',\n",
       " 'EML1_081-Variables5',\n",
       " 'EML1_097-Bias2',\n",
       " 'EML1_108-Validity7',\n",
       " 'EML1_056-Variables5',\n",
       " 'EML1_100-Hypotheses4',\n",
       " 'EML1_099-Bias7',\n",
       " 'EML1_016-Hypotheses5',\n",
       " 'EML1_101-Variables8',\n",
       " 'EML1_074-Validity1',\n",
       " 'EML1_048-Bias8',\n",
       " 'EML1_024-CausalClaims2',\n",
       " 'EML1_076-CausalClaims2',\n",
       " 'EML1_104-CausalClaims2',\n",
       " 'EML1_110-Validity1',\n",
       " 'EML1_108-Bias8',\n",
       " 'EML1_118-CausalClaims2',\n",
       " 'EML1_032-Variables1',\n",
       " 'EML1_133-Validity8',\n",
       " 'EML1_114-Hypotheses4',\n",
       " 'EML1_039-Hypotheses1',\n",
       " 'EML1_026-Variables5',\n",
       " 'EML1_011-Hypotheses1',\n",
       " 'EML1_114-CausalClaims3',\n",
       " 'EML1_048-CausalClaims6',\n",
       " 'EML1_087-Validity1',\n",
       " 'EML1_101-Variables1',\n",
       " 'EML1_090-Hypotheses1',\n",
       " 'EML1_004-Hypotheses1',\n",
       " 'EML1_098-Variables1',\n",
       " 'EML1_026-Validity1',\n",
       " 'EML1_112-Validity8',\n",
       " 'EML1_156-Bias6',\n",
       " 'EML1_104-Hypotheses4',\n",
       " 'EML1_141-Validity8',\n",
       " 'EML1_046-CausalClaims8',\n",
       " 'EML1_085-Validity1',\n",
       " 'EML1_135-Variables8',\n",
       " 'EML1_064-Hypotheses1',\n",
       " 'EML1_003-Hypotheses8',\n",
       " 'EML1_113-Bias6',\n",
       " 'EML1_075-Hypotheses4',\n",
       " 'EML1_026-Hypotheses8',\n",
       " 'EML1_103-Validity1',\n",
       " 'EML1_066-Variables5',\n",
       " 'EML1_125-Bias7',\n",
       " 'EML1_067-CausalClaims3',\n",
       " 'EML1_103-CausalClaims8',\n",
       " 'EML1_006-Validity7',\n",
       " 'EML1_063-Variables8',\n",
       " 'EML1_066-Variables8',\n",
       " 'EML1_007-Bias2',\n",
       " 'EML1_070-Hypotheses5',\n",
       " 'EML1_132-Validity1',\n",
       " 'EML1_049-Validity8',\n",
       " 'EML1_126-Hypotheses8',\n",
       " 'EML1_027-Hypotheses1',\n",
       " 'EML1_094-Validity1',\n",
       " 'EML1_082-Validity8',\n",
       " 'EML1_031-Variables5',\n",
       " 'EML1_004-CausalClaims2',\n",
       " 'EML1_094-CausalClaims8',\n",
       " 'EML1_136-Variables3',\n",
       " 'EML1_033-Validity8',\n",
       " 'EML1_093-Hypotheses1',\n",
       " 'EML1_072-CausalClaims3',\n",
       " 'EML1_035-Variables1',\n",
       " 'EML1_022-Validity8',\n",
       " 'EML1_048-Variables1',\n",
       " 'EML1_147-Variables3',\n",
       " 'EML1_046-Validity6',\n",
       " 'EML1_059-Hypotheses5',\n",
       " 'EML1_117-CausalClaims2',\n",
       " 'EML1_085-CausalClaims8',\n",
       " 'EML1_095-Hypotheses4',\n",
       " 'EML1_040-Validity7',\n",
       " 'EML1_067-Hypotheses8',\n",
       " 'EML1_048-Bias2',\n",
       " 'EML1_097-Variables3',\n",
       " 'EML1_082-CausalClaims3',\n",
       " 'EML1_020-Hypotheses8',\n",
       " 'EML1_067-Validity6',\n",
       " 'EML1_025-Hypotheses4',\n",
       " 'EML1_060-CausalClaims2',\n",
       " 'EML1_037-CausalClaims6',\n",
       " 'EML1_046-Bias6',\n",
       " 'EML1_086-CausalClaims6',\n",
       " 'EML1_081-Bias7',\n",
       " 'EML1_127-Validity6',\n",
       " 'EML1_103-Validity7',\n",
       " 'EML1_005-Hypotheses8',\n",
       " 'EML1_006-CausalClaims2',\n",
       " 'EML1_067-Variables3',\n",
       " 'EML1_022-Validity1',\n",
       " 'EML1_104-Variables8',\n",
       " 'EML1_123-Variables8',\n",
       " 'EML1_111-Hypotheses4',\n",
       " 'EML1_076-Bias2',\n",
       " 'EML1_027-Hypotheses5',\n",
       " 'EML1_005-Bias8',\n",
       " 'EML1_135-Hypotheses4',\n",
       " 'EML1_118-Bias6',\n",
       " 'EML1_066-CausalClaims8',\n",
       " 'EML1_041-Validity6',\n",
       " 'EML1_125-CausalClaims8',\n",
       " 'EML1_086-Bias2',\n",
       " 'EML1_092-Variables3',\n",
       " 'EML1_106-Validity6',\n",
       " 'EML1_071-Variables1',\n",
       " 'EML1_075-CausalClaims2',\n",
       " 'EML1_091-CausalClaims6',\n",
       " 'EML1_056-Hypotheses1',\n",
       " 'EML1_045-Bias6',\n",
       " 'EML1_115-Hypotheses1',\n",
       " 'EML1_038-Hypotheses5',\n",
       " 'EML1_066-Hypotheses8',\n",
       " 'EML1_092-Hypotheses5',\n",
       " 'EML1_108-Variables3',\n",
       " 'EML1_015-CausalClaims8',\n",
       " 'EML1_003-Validity1',\n",
       " 'EML1_084-Hypotheses1',\n",
       " 'EML1_047-CausalClaims3',\n",
       " 'EML1_129-Bias6',\n",
       " 'EML1_045-CausalClaims3',\n",
       " 'EML1_060-Hypotheses5',\n",
       " 'EML1_046-Validity1',\n",
       " 'EML1_134-Validity1',\n",
       " 'EML1_113-Variables1',\n",
       " 'EML1_017-CausalClaims6',\n",
       " 'EML1_051-CausalClaims3',\n",
       " 'EML1_130-CausalClaims8',\n",
       " 'EML1_070-Validity6',\n",
       " 'EML1_022-Bias6',\n",
       " 'EML1_119-Hypotheses5',\n",
       " 'EML1_123-Variables3',\n",
       " 'EML1_030-Hypotheses8',\n",
       " 'EML1_047-CausalClaims8',\n",
       " 'EML1_010-Variables8',\n",
       " 'EML1_078-Validity1',\n",
       " 'EML1_042-Hypotheses8',\n",
       " 'EML1_040-Bias2',\n",
       " 'EML1_081-Validity7',\n",
       " 'EML1_029-Bias8',\n",
       " 'EML1_041-Validity1',\n",
       " 'EML1_110-Validity8',\n",
       " 'EML1_125-Validity1',\n",
       " 'EML1_056-CausalClaims8',\n",
       " 'EML1_058-CausalClaims3',\n",
       " 'EML1_113-Hypotheses1',\n",
       " 'EML1_119-Bias7',\n",
       " 'EML1_142-CausalClaims8',\n",
       " 'EML1_079-Variables3',\n",
       " 'EML1_046-Bias2',\n",
       " 'EML1_064-CausalClaims6',\n",
       " 'EML1_118-Variables8',\n",
       " 'EML1_113-CausalClaims3',\n",
       " 'EML1_059-Variables5',\n",
       " 'EML1_093-Bias6',\n",
       " 'EML1_129-CausalClaims8',\n",
       " 'EML1_081-Validity1',\n",
       " 'EML1_042-Validity6',\n",
       " 'EML1_134-Hypotheses8',\n",
       " 'EML1_106-CausalClaims6',\n",
       " 'EML1_007-Hypotheses8',\n",
       " 'EML1_119-Variables8',\n",
       " 'EML1_028-Variables3',\n",
       " 'EML1_083-Hypotheses4',\n",
       " 'EML1_004-CausalClaims8',\n",
       " 'EML1_128-Hypotheses8',\n",
       " 'EML1_036-Hypotheses4',\n",
       " 'EML1_064-Bias2',\n",
       " 'EML1_093-Bias2',\n",
       " 'EML1_069-Validity1',\n",
       " 'EML1_064-CausalClaims2',\n",
       " 'EML1_080-Bias2',\n",
       " 'EML1_059-Bias6',\n",
       " 'EML1_124-CausalClaims8',\n",
       " 'EML1_111-Validity8',\n",
       " 'EML1_033-Bias7',\n",
       " 'EML1_099-Validity1',\n",
       " 'EML1_012-CausalClaims6',\n",
       " 'EML1_132-Hypotheses5',\n",
       " 'EML1_120-CausalClaims8',\n",
       " 'EML1_135-Variables3',\n",
       " 'EML1_010-Validity1',\n",
       " 'EML1_032-CausalClaims8',\n",
       " 'EML1_092-CausalClaims3',\n",
       " 'EML1_011-CausalClaims2',\n",
       " 'EML1_052-Validity1',\n",
       " 'EML1_115-CausalClaims6',\n",
       " 'EML1_015-CausalClaims2',\n",
       " 'EML1_021-Hypotheses8',\n",
       " 'EML1_042-CausalClaims2',\n",
       " 'EML1_125-Variables5',\n",
       " 'EML1_122-Validity8',\n",
       " 'EML1_133-Variables8',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Splits\n",
    "splits = get_stratified_group_splits(files, label_df, l_ds[0][0], \"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one split\n",
    "train_split, val_split = next(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    2,    3,    5,    6,    7,    9,   10,   11,   13,   14,\n",
       "         15,   17,   18,   19,   21,   22,   23,   25,   26,   27,   29,\n",
       "         30,   31,   32,   34,   35,   37,   38,   39,   41,   42,   43,\n",
       "         45,   46,   47,   49,   50,   51,   53,   54,   55,   56,   58,\n",
       "         59,   61,   62,   63,   64,   65,   66,   69,   70,   71,   73,\n",
       "         74,   75,   76,   78,   79,   81,   82,   83,   84,   86,   87,\n",
       "         88,   90,   91,   93,   94,   95,   97,   98,   99,  100,  102,\n",
       "        103,  104,  106,  107,  109,  110,  111,  113,  114,  115,  117,\n",
       "        118,  119,  121,  122,  123,  125,  126,  127,  128,  130,  131,\n",
       "        132,  133,  135,  136,  138,  139,  141,  142,  143,  145,  146,\n",
       "        147,  148,  150,  151,  153,  154,  155,  157,  158,  159,  160,\n",
       "        162,  163,  165,  166,  167,  168,  170,  171,  173,  174,  175,\n",
       "        176,  178,  179,  181,  182,  183,  185,  186,  187,  189,  190,\n",
       "        191,  193,  194,  195,  197,  198,  199,  201,  202,  203,  205,\n",
       "        206,  207,  208,  210,  211,  213,  214,  215,  217,  218,  219,\n",
       "        221,  222,  223,  225,  226,  227,  229,  230,  231,  232,  234,\n",
       "        235,  237,  238,  239,  241,  242,  243,  245,  246,  247,  248,\n",
       "        249,  251,  253,  254,  255,  257,  258,  259,  260,  262,  263,\n",
       "        265,  266,  267,  269,  270,  271,  273,  274,  275,  277,  278,\n",
       "        279,  281,  282,  283,  285,  286,  287,  288,  290,  291,  293,\n",
       "        294,  295,  297,  298,  299,  301,  302,  303,  305,  306,  307,\n",
       "        308,  310,  311,  313,  314,  315,  316,  317,  319,  321,  322,\n",
       "        323,  324,  326,  327,  328,  330,  331,  332,  334,  335,  336,\n",
       "        338,  339,  341,  342,  343,  345,  346,  347,  349,  350,  351,\n",
       "        352,  354,  355,  356,  358,  359,  360,  362,  363,  365,  366,\n",
       "        367,  369,  370,  371,  372,  374,  375,  376,  377,  379,  380,\n",
       "        381,  383,  385,  386,  387,  388,  390,  391,  393,  394,  395,\n",
       "        397,  398,  399,  401,  402,  403,  404,  406,  407,  409,  410,\n",
       "        411,  413,  414,  415,  416,  418,  419,  421,  422,  423,  424,\n",
       "        426,  427,  428,  430,  431,  432,  433,  435,  436,  437,  439,\n",
       "        441,  442,  443,  445,  446,  447,  448,  450,  451,  453,  454,\n",
       "        455,  457,  458,  459,  461,  462,  463,  465,  466,  467,  469,\n",
       "        470,  471,  473,  474,  475,  476,  478,  479,  481,  482,  483,\n",
       "        485,  486,  487,  489,  490,  491,  492,  494,  495,  497,  498,\n",
       "        499,  501,  502,  503,  504,  506,  507,  508,  510,  511,  513,\n",
       "        514,  515,  516,  518,  519,  521,  522,  523,  525,  526,  527,\n",
       "        529,  530,  531,  533,  534,  535,  537,  538,  539,  540,  542,\n",
       "        543,  545,  546,  547,  548,  549,  551,  553,  554,  555,  557,\n",
       "        558,  559,  560,  562,  563,  564,  566,  567,  568,  570,  571,\n",
       "        572,  574,  575,  576,  578,  579,  581,  582,  583,  584,  586,\n",
       "        587,  589,  590,  591,  592,  593,  595,  597,  598,  599,  601,\n",
       "        602,  603,  605,  606,  607,  609,  610,  611,  613,  614,  615,\n",
       "        617,  618,  619,  621,  622,  623,  625,  626,  627,  629,  630,\n",
       "        631,  633,  634,  635,  636,  637,  639,  640,  641,  643,  644,\n",
       "        645,  647,  648,  649,  651,  653,  654,  655,  656,  657,  659,\n",
       "        661,  662,  663,  664,  666,  667,  668,  670,  671,  672,  674,\n",
       "        675,  677,  678,  679,  681,  682,  683,  685,  686,  687,  688,\n",
       "        690,  691,  693,  694,  695,  697,  698,  699,  700,  701,  703,\n",
       "        704,  705,  707,  708,  709,  710,  713,  714,  715,  716,  718,\n",
       "        719,  720,  722,  723,  724,  726,  727,  729,  730,  731,  733,\n",
       "        734,  735,  737,  738,  739,  741,  742,  743,  744,  746,  747,\n",
       "        748,  750,  751,  752,  754,  755,  757,  758,  759,  761,  762,\n",
       "        763,  765,  766,  767,  769,  770,  771,  773,  774,  775,  777,\n",
       "        778,  779,  781,  782,  783,  785,  786,  787,  788,  790,  791,\n",
       "        792,  793,  795,  796,  797,  799,  800,  801,  803,  805,  806,\n",
       "        807,  808,  810,  811,  813,  814,  815,  817,  818,  819,  821,\n",
       "        822,  823,  824,  826,  827,  829,  830,  831,  832,  834,  835,\n",
       "        837,  838,  839,  841,  842,  843,  845,  846,  847,  848,  850,\n",
       "        851,  853,  854,  855,  857,  858,  859,  861,  862,  863,  865,\n",
       "        866,  867,  869,  870,  871,  872,  874,  875,  876,  878,  879,\n",
       "        881,  882,  883,  884,  886,  887,  888,  890,  891,  892,  893,\n",
       "        895,  897,  898,  899,  900,  901,  903,  904,  906,  907,  908,\n",
       "        910,  911,  913,  914,  915,  917,  918,  919,  921,  922,  923,\n",
       "        925,  926,  927,  929,  930,  931,  933,  934,  935,  937,  938,\n",
       "        939,  940,  942,  943,  945,  946,  947,  949,  950,  951,  952,\n",
       "        953,  955,  956,  958,  959,  960,  962,  963,  964,  965,  967,\n",
       "        968,  970,  971,  972,  974,  975,  977,  978,  979,  981,  982,\n",
       "        983,  984,  985,  987,  988,  990,  991,  992,  994,  995,  997,\n",
       "        998,  999, 1001, 1002, 1003, 1004, 1006, 1007, 1008, 1010, 1011,\n",
       "       1012, 1014, 1015, 1016, 1017, 1019, 1021, 1022, 1023, 1025, 1026,\n",
       "       1027, 1029, 1030, 1031, 1032, 1034, 1035, 1036, 1038, 1039, 1041,\n",
       "       1042, 1043, 1044, 1046, 1047, 1048, 1050, 1051, 1053, 1054, 1055,\n",
       "       1057, 1058, 1059, 1061, 1062, 1063, 1065, 1066, 1067, 1069, 1070,\n",
       "       1071, 1073, 1074, 1075, 1077, 1078, 1079, 1080, 1082, 1083, 1084,\n",
       "       1085, 1087, 1089, 1090, 1091, 1092, 1094, 1095, 1096, 1097, 1098,\n",
       "       1101, 1102, 1103, 1105, 1106, 1107, 1109, 1110, 1111, 1112, 1114,\n",
       "       1115, 1116, 1117, 1119, 1120, 1121, 1123, 1124, 1125, 1127, 1128,\n",
       "       1129, 1130, 1132, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1142,\n",
       "       1144, 1145, 1147, 1148, 1150, 1151, 1153, 1154, 1155, 1157, 1158,\n",
       "       1159, 1161, 1162, 1163, 1165, 1166, 1167, 1169, 1170, 1171, 1172,\n",
       "       1174, 1175, 1176, 1178, 1179, 1181, 1182, 1183, 1185, 1186, 1187,\n",
       "       1189, 1190, 1191, 1192, 1194, 1195, 1197, 1198, 1199, 1201, 1202,\n",
       "       1203, 1205, 1206, 1207, 1209, 1210, 1211, 1213, 1214, 1215, 1217,\n",
       "       1218, 1219, 1221, 1222, 1223, 1224, 1226, 1227, 1228, 1230, 1231,\n",
       "       1232, 1234, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1245, 1246,\n",
       "       1247, 1249, 1250])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251 1251\n"
     ]
    }
   ],
   "source": [
    "# Setup datamodule\n",
    "dm = l_ds[0][1]\n",
    "dm.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloader\n",
    "def get_dataloaders_from_split(dm, train_split, val_split):\n",
    "    train_sampler = SubsetRandomSampler(train_split)\n",
    "    train_dl = dm.train_dataloader(sampler=train_sampler)\n",
    "    val_sampler = SubsetRandomSampler(val_split)\n",
    "    val_dl = dm.val_dataloader(sampler=val_sampler)\n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = get_dataloaders_from_split(dm, train_split, val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_weights_dir = Path(\"../OBF/pre_weights/sample_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained encoder\n",
    "encoder = creator.load_encoder(str(pre_trained_weights_dir.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderClassifierModel(encoder, cuda=False, freeze_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"Rote_X_NotFreeze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(max_epochs=10, logger=logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 2/156 [00:04<05:24,  2.11s/it, loss=0.72, v_num=0, train_loss_step=0.741, train_accuracy_step=0.625]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 156/156 [03:50<00:00,  1.48s/it, loss=0.563, v_num=0, train_loss_step=0.626, train_accuracy_step=0.750, val_loss_step=0.474, val_accuracy_step=0.875, val_loss_epoch=0.579, val_accuracy_epoch=0.734, val_auroc=0.535, train_loss_epoch=0.606, train_accuracy_epoch=0.710, train_auroc=0.504]\n"
     ]
    }
   ],
   "source": [
    "# Find learning rate\n",
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"Rote_X\",\"Inference_X\", \"Deep_X\", \"Rote_Y\", \"Inference_Y\", \"Rote_Z\", \"Inference_Z\", \"Deep_Z\", \"Rote_D\", \"Inference_D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ds = get_datasets(label_cols, label_df, data_folder, x_transforms=[limit_sequence_len,lambda data: torch.tensor(data).float()], y_transforms=[lambda data: torch.tensor(data).float()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_inf_y = l_ds[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235 1235\n"
     ]
    }
   ],
   "source": [
    "dm_inf_y.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm_inf_y.dataset_train.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = get_all_splits(l_ds, label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s, val_s = next(all_splits[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory lightning_logs/Rote_X_NotFreeze/version_0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▌         | 9/154 [00:20<05:33,  2.30s/it, loss=0.737, v_num=0, train_loss_step=0.733, train_accuracy_step=0.625]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  18%|█▊        | 27/154 [01:03<04:59,  2.36s/it, loss=0.682, v_num=0, train_loss_step=0.660, train_accuracy_step=0.625]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "train_one_split(l_ds[4][1], train_s, val_s, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_split(dm, train_split, val_split, max_epochs, logger):\n",
    "    dm.setup(stage=\"fit\")\n",
    "    train_dl, val_dl = get_dataloaders_from_split(dm, train_split, val_split)\n",
    "    encoder = creator.load_encoder(str(pre_trained_weights_dir.resolve()))\n",
    "    model = EncoderClassifierModel(encoder, cuda=False, freeze_encoder=True)\n",
    "    trainer=Trainer(max_epochs=max_epochs, logger=logger)\n",
    "    trainer.fit(model, train_dl, val_dl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "def train_multiple_dms(dms, splits, label_cols, epochs=5):\n",
    "    for split, dm, label_col in zip(splits, dms, label_cols):\n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name=label_col)\n",
    "        train_split, val_split = next(split)\n",
    "        train_one_split(dm, train_split, val_split, epochs, logger)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n",
      "Epoch 0:  18%|█▊        | 27/154 [05:32<26:03, 12.31s/it, loss=0.682, v_num=0, train_loss_step=0.660, train_accuracy_step=0.625] train_loss_epoch=0.725, train_accuracy_epoch=0.542, train_auroc=0.461]\n",
      "Epoch 4:  33%|███▎      | 51/154 [01:45<03:33,  2.07s/it, loss=0.585, v_num=0, train_loss_step=0.893, train_accuracy_step=0.375, train_loss_epoch=0.593, train_accuracy_epoch=0.717, train_auroc=0.466]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268 1268\n",
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 6/157 [00:11<04:47,  1.90s/it, loss=0.756, v_num=0, train_loss_step=0.772, train_accuracy_step=0.625]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  15%|█▌        | 24/157 [00:46<04:18,  1.94s/it, loss=0.713, v_num=0, train_loss_step=0.742, train_accuracy_step=0.375]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [04:27<00:00,  1.71s/it, loss=0.67, v_num=0, train_loss_step=0.756, train_accuracy_step=0.375, val_loss_step=0.696, val_accuracy_step=0.625, val_loss_epoch=0.634, val_accuracy_epoch=0.667, val_auroc=0.539, train_loss_epoch=0.666, train_accuracy_epoch=0.606, train_auroc=0.517] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268 1268\n",
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 4/157 [00:09<06:20,  2.49s/it, loss=0.659, v_num=0, train_loss_step=0.721, train_accuracy_step=0.500]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [04:21<00:00,  1.67s/it, loss=0.667, v_num=0, train_loss_step=0.903, train_accuracy_step=0.375, val_loss_step=0.604, val_accuracy_step=0.750, val_loss_epoch=0.577, val_accuracy_epoch=0.728, val_auroc=0.551, train_loss_epoch=0.624, train_accuracy_epoch=0.700, train_auroc=0.429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268 1268\n",
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  65%|██████▍   | 102/157 [03:34<01:55,  2.10s/it, loss=0.714, v_num=0, train_loss_step=0.843, train_accuracy_step=0.375]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it, loss=0.671, v_num=0, train_loss_step=0.725, train_accuracy_step=0.750, val_loss_step=0.674, val_accuracy_step=0.625, val_loss_epoch=0.673, val_accuracy_epoch=0.590, val_auroc=0.586, train_loss_epoch=0.705, train_accuracy_epoch=0.549, train_auroc=0.530]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041 1041\n",
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  13%|█▎        | 17/129 [00:49<05:24,  2.90s/it, loss=0.69, v_num=0, train_loss_step=0.798, train_accuracy_step=0.250] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  57%|█████▋    | 74/129 [03:12<02:23,  2.61s/it, loss=0.686, v_num=0, train_loss_step=0.561, train_accuracy_step=0.625, train_loss_epoch=0.689, train_accuracy_epoch=0.635, train_auroc=0.485]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | Sequential        | 362 K \n",
      "1 | criterion       | BCEWithLogitsLoss | 0     \n",
      "2 | auroc_metric    | AUROC             | 0     \n",
      "3 | accuracy_metric | Accuracy          | 0     \n",
      "------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "0         Non-trainable params\n",
      "362 K     Total params\n",
      "1.451     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045 1045\n",
      "Loading:  /Users/rickgentry/emotive_lab/eyemind/OBF/pre_weights/sample_weights/encoder_1633040995_gru.pt\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  30%|███       | 39/130 [01:39<03:51,  2.54s/it, loss=0.691, v_num=0, train_loss_step=0.837, train_accuracy_step=0.250]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rickgentry/miniforge3/envs/cm2/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 130/130 [04:27<00:00,  2.06s/it, loss=0.727, v_num=0, train_loss_step=0.709, train_accuracy_step=0.625, val_loss_step=0.456, val_accuracy_step=1.000, val_loss_epoch=0.664, val_accuracy_epoch=0.629, val_auroc=0.396, train_loss_epoch=0.701, train_accuracy_epoch=0.570, train_auroc=0.445]\n"
     ]
    }
   ],
   "source": [
    "dms = [t[1] for t in l_ds]\n",
    "train_multiple_dms(dms[4:], all_splits[4:], label_cols[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1a371005750433fad728c36ce8654a34688417482f17f4469971f4dff2fd726"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('cm2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
