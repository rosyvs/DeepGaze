{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from eyemind.trainer.loops import KFoldLoop\n",
    "import eyemind\n",
    "from eyemind.models.transformers import InformerEncoderDecoderModel, InformerEncoderFixationModel, InformerMultiTaskEncoderDecoder\n",
    "from eyemind.dataloading.informer_data import InformerDataModule, InformerMultiLabelDatamodule,  InformerVariableLengthDataModule\n",
    "import matplotlib.pyplot as plt\n",
    "from eyemind.analysis.visualize import plot_scanpath_labels, viz_coding, fixation_image, plot_scanpath_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Simply load in already-trained models and generate predictions\n",
    "\n",
    "This shouldn't be this difficult!\n",
    "\n",
    "# Notes\n",
    "\n",
    "\n",
    "in the original informer code, there is a command line option\n",
    "`parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')`\n",
    "Check what this does\n",
    "\n",
    "## encoder only\n",
    "I have managed to get logits from the encoder, but model(batch) does not work when mdoel is the entire multitask enc-decoder stack.\n",
    "\n",
    "## fixation decoder\n",
    "How to load this? \n",
    "A: model.fi_decoder\n",
    "\n",
    "# TODO ideas\n",
    "[x] try loading encoder and fi decoder separately\n",
    "\n",
    "[x] get predictions for other tasks\n",
    "\n",
    "[x] plot predictions\n",
    "\n",
    "[ ] plot attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n"
     ]
    }
   ],
   "source": [
    "# Load our trained encoder decoder from checkpoint for each fold\n",
    "# this shouold load the encoder and its 4 decoders\n",
    "\n",
    "# using pytorch lightning module\n",
    "# https://lightning.ai/docs/pytorch/stable/deploy/production_basic.html\n",
    "\n",
    "repodir = os.path.dirname(os.path.dirname(eyemind.__file__))\n",
    "test_data_dir = os.path.join(repodir,\"data/EML/gaze+fix+reg\")\n",
    "test_label_file =  os.path.join(repodir,\"./data/EML/EML1_pageLevel_500+_matchEDMinstances.csv\")\n",
    "# save_dir_base = f\"{repodir}/lightning_logs/2024/cluster/new_multitask_informer_pretraining\"\n",
    "save_dir_base = f\"{repodir}/lightning_logs/informer_pretraining_seed21\"\n",
    "\n",
    "is_old_version=True # i.e. before I made changes to model and datamodule to support multiple labels etc\n",
    "fold=0\n",
    "save_dir = os.path.join(save_dir_base, f'fold{fold}/')\n",
    "config_path=os.path.join(save_dir,\"config.yaml\")\n",
    "ckpt_path = os.path.join(save_dir,\"checkpoints\",\"last.ckpt\")\n",
    "# ckpt_path = os.path.join(save_dir,\"checkpoints\",\"epoch=168-step=8619.ckpt\")\n",
    "\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "seed_everything(config[\"seed_everything\"], workers=True) # not sure if this is needed\n",
    "\n",
    "model = InformerMultiTaskEncoderDecoder.load_from_checkpoint(ckpt_path,\n",
    "                                                # encoder_weights_path=None\n",
    "                                                )\n",
    "encoder=model.encoder\n",
    "# decoder=model.fi_decoder\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "# decoder.eval()\n",
    "\n",
    "# set up an InformerDataModule to load the same data as used in training\n",
    "# trainer = Trainer(**config[\"trainer\"])\n",
    "# data_dir = os.path.join(repodir,config[\"data\"][\"data_dir\"])\n",
    "# label_file = os.path.join(repodir,config[\"data\"][\"label_filepath\"])\n",
    "config[\"data\"][\"data_dir\"]=test_data_dir\n",
    "config[\"data\"][\"label_filepath\"]=test_label_file\n",
    "\n",
    "# edit config for consistency woth the new version of the datamodule\n",
    "if is_old_version:\n",
    "    config[\"data\"][\"min_sequence_length\"] = config[\"data\"][\"min_scanpath_length\"]\n",
    "    config[\"data\"].pop(\"min_scanpath_length\")\n",
    "    config[\"data\"][\"sample_label_col\"]=\"fixation_label\"\n",
    "    config[\"data\"][\"file_label_col\"]=None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datamodule = InformerMultiLabelDatamodule(**config[\"data\"])\n",
    "datamodule.setup()\n",
    "\n",
    "test_dl = datamodule.get_dataloader(datamodule.test_dataset) # this is the held out fold's dataloader\n",
    "for i,batch in enumerate(test_dl):\n",
    "    print(f\"batch: {i}\")\n",
    "    print(f\"length of batch: {len(batch)}\")\n",
    "    n_items = len(batch) # this is not the batch size, but the number of items (data and labels) to unpack\n",
    "    if n_items==2: # just gaze sequence and fixation (sample) labels\n",
    "        X, yi = batch\n",
    "        X, X_mask = X \n",
    "        yi, yi_mask = yi\n",
    "    elif n_items==4: # contrastive so X and X2 are present\n",
    "        X, yi, X2, cl_y = batch\n",
    "        X, X_mask = X \n",
    "        yi, yi_mask = yi\n",
    "        X2, X2_mask = X2\n",
    "    elif n_items==5: # sequence and fixation (sample) labels\n",
    "        X, yi, seq_y, X2, cl_y = batch\n",
    "        X, X_mask = X\n",
    "        yi, yi_mask = yi\n",
    "        seq_y, seq_y_mask = seq_y\n",
    "        X2, X2_mask = X2\n",
    "    with torch.no_grad():\n",
    "        logits=encoder(X, None)\n",
    "\n",
    "\n",
    "    if i==0: # just run a couple to check\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perc_nan = torch.isnan(logits).sum() / torch.numel(logits)\n",
    "print(f'percentage of nans in  encoder logits: {100*perc_nan}')\n",
    "# if torch.equal(logits2, logits):\n",
    "#     print('providing labels made no difference')\n",
    "# else:\n",
    "#     print('removing labels changed the predicitons')\n",
    "\n",
    "print(f'logits shape: {logits.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits[0,:50,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can see the attributes of the full model using dir\n",
    "# dir(model)\n",
    "# # it has a module list of decoders\n",
    "# dir(model.decoders)\n",
    "if hasattr(model, \"fi_decoder\"):\n",
    "    print(\"model has fi_decoder\")\n",
    "elif hasattr(model, \"fm_decoder\"):\n",
    "    model.fi_decoder = model.fm_decoder\n",
    "    print(\"model has fm_decoder\")\n",
    "else:\n",
    "    print(\"model has no fixation decoder\")\n",
    "# # Apart from pc we can access each task's decoder like so:\n",
    "# print(f'FI decoder: {model.fi_decoder}')\n",
    "# print(f'RC decoder: {model.rc_decoder}')\n",
    "# print(f'CL decoder: {model.cl_decoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! Let's use the logits from the encoder as input to the fi decoder...\n",
    "\n",
    "fixation_logits = model.fi_decoder.forward(logits)\n",
    "# this is of shape n, len, 2. I assume the 2 dimensions here are prob(fix) and prob(sacc)? \n",
    "# do we need to just take one dimension? And softmax then threshold >/5?\n",
    "fixation_preds=fixation_logits.max(2).indices\n",
    "print(fixation_logits)\n",
    "fixation_targets = yi\n",
    "# how many unique values are there in the targets?\n",
    "print(f\"unique preds: {torch.unique(fixation_preds)}\")\n",
    "print(f\"unique targets: {torch.unique(fixation_targets)}\")\n",
    "# or reshape batch into one long vector in Ricks's code, to get batch-wisemetric:\n",
    "logits_long = fixation_logits.squeeze().reshape(-1,2)\n",
    "targets_long = fixation_targets.reshape(-1).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pick one from batch to plot\n",
    "ix=4\n",
    "one_pred = fixation_preds[ix,:]\n",
    "one_target = fixation_targets[ix,:]\n",
    "print(one_pred.shape)\n",
    "print(one_target.shape)\n",
    "fixation_image(one_pred, one_target, \"Fixation Identification - Informer (top:pred, bottom:target)\")\n",
    "\n",
    "# SUCCESS!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate FI \n",
    "\n",
    "#mask = torch.any(X == -180, dim=1)\n",
    "loss = model.fi_criterion(logits_long, targets_long)\n",
    "preds = model._get_preds(logits)\n",
    "probs = model._get_probs(logits)\n",
    "fixation_targets = fixation_targets.int()\n",
    "auprc = model.fi_metric(probs, fixation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictive coding one batch\n",
    "from eyemind.dataloading.batch_loading import predictive_coding_batch\n",
    "\n",
    "pred_length = 150\n",
    "label_length = 100\n",
    "pc_seq_length = 350\n",
    "X_pc, Y_pc = predictive_coding_batch(X, pc_seq_length=pc_seq_length, label_length=label_length, pred_length=pred_length)\n",
    "with torch.no_grad():\n",
    "    pc_logits = model.pc_decoder.forward(logits, Y_pc, pred_length=pred_length).squeeze()\n",
    "pc_target=Y_pc[:,:-pred_length] # take just the predicted part as target\n",
    "# coutn how many nan in pc_logits\n",
    "nan_count = torch.isnan(pc_logits).sum()\n",
    "# print(f\"Number of nan in pc_logits: {nan_count} / {pc_logits.numel()}\")\n",
    "print(f\"PC logits: {pc_logits.shape}\")\n",
    "perc_nan = nan_count/pc_logits.numel()\n",
    "print(f\"Percentage of nan in PC logits: {100*perc_nan:.2f}\")\n",
    "pc_logits = model.scaler.inverse_transform(pc_logits)\n",
    "pc_target = model.scaler.inverse_transform(pc_target)\n",
    "X = model.scaler.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=9\n",
    "\n",
    "handle = plot_scanpath_pc(X[ix,:,0], X[ix,:,1], pc_logits[ix,:,0].cpu().detach().numpy(), pc_logits[ix,:,1].cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction\n",
    "with torch.no_grad():\n",
    "    recon_logits = model.rc_decoder.forward(logits, X)\n",
    "recon_targets = X\n",
    "\n",
    "perc_nan = torch.isnan(recon_logits).sum()/recon_logits.numel()\n",
    "print(f\"Percentage of nan in recon_logits: {100*perc_nan:.2f}\")\n",
    "\n",
    "# plot using pc function\n",
    "ix=2\n",
    "handle = plot_scanpath_pc(recon_targets[ix,:,0], recon_targets[ix,:,1], recon_logits[ix,:,0].cpu().detach().numpy(), recon_logits[ix,:,1].cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': '/home/rosy/DeepGaze/data/EML/gaze+fix+reg',\n",
       " 'label_filepath': '/home/rosy/DeepGaze/./data/EML/EML1_pageLevel_500+_matchEDMinstances.csv',\n",
       " 'load_setup_path': None,\n",
       " 'test_dir': None,\n",
       " 'train_dataset': None,\n",
       " 'val_dataset': None,\n",
       " 'test_dataset': None,\n",
       " 'train_fold': None,\n",
       " 'val_fold': None,\n",
       " 'num_workers': 4,\n",
       " 'batch_size': 32,\n",
       " 'pin_memory': True,\n",
       " 'drop_last': True,\n",
       " 'min_sequence_length': 10,\n",
       " 'max_sequence_length': 2000,\n",
       " 'label_col': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataloader with variable sequence length and padding \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "length of batch: 2\n",
      "torch.Size([32, 2000, 2])\n",
      "False\n",
      "percentage of nans in  encoder logits: 0.00\n",
      "percentage of nans in  encoder logits from masked: 0.00\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7113,  0.0050,  0.4600,  ...,  0.0159, -0.0166,  0.1135],\n",
       "         [-0.2500, -0.1553,  0.1488,  ..., -0.1673,  0.0022,  0.2387],\n",
       "         [-0.2646, -0.1868,  0.2355,  ..., -0.2032,  0.0284,  0.2396],\n",
       "         ...,\n",
       "         [-0.2758,  0.0041,  0.4808,  ..., -0.1571,  0.1564,  0.2501],\n",
       "         [-0.3866, -0.1225,  0.3696,  ..., -0.1294,  0.1486,  0.2532],\n",
       "         [ 0.3169,  0.0255,  1.1996,  ..., -0.4156,  0.1024,  0.2111]],\n",
       "\n",
       "        [[-0.6587, -0.0388,  1.5289,  ..., -0.2655,  0.1047,  0.0391],\n",
       "         [-0.7262, -0.1766,  0.4579,  ..., -0.2528,  0.1531,  0.2002],\n",
       "         [-0.7316, -0.1942,  0.4682,  ..., -0.2596,  0.1536,  0.1992],\n",
       "         ...,\n",
       "         [-0.7570,  0.1429,  1.1773,  ..., -0.1421, -0.1104,  0.1763],\n",
       "         [-0.8990, -0.2138,  0.6514,  ..., -0.2344,  0.1967,  0.1939],\n",
       "         [-0.6549, -0.0407,  1.5165,  ..., -0.3282,  0.1854,  0.0593]],\n",
       "\n",
       "        [[ 0.2507,  0.0743,  0.7018,  ..., -0.3943, -0.2530,  0.1455],\n",
       "         [-0.3186, -0.0794,  0.2028,  ..., -0.4221, -0.0933,  0.3598],\n",
       "         [-0.2393, -0.1167,  0.4950,  ..., -0.5706,  0.0536,  0.3480],\n",
       "         ...,\n",
       "         [-0.2857, -0.1479,  0.2091,  ..., -0.3215, -0.0927,  0.3678],\n",
       "         [-0.2864, -0.1532,  0.1875,  ..., -0.3305, -0.0854,  0.3654],\n",
       "         [ 0.4898,  0.0071,  0.5658,  ..., -0.4171, -0.3655,  0.1864]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2620,  0.0530,  0.0467,  ..., -0.2810, -0.1107,  0.2050],\n",
       "         [-0.3236, -0.0432,  0.1516,  ..., -0.2777, -0.2332,  0.2142],\n",
       "         [-0.3258, -0.0559,  0.1530,  ..., -0.2909, -0.2259,  0.2119],\n",
       "         ...,\n",
       "         [-0.2431, -0.0122,  0.2851,  ..., -0.3484, -0.1962,  0.1992],\n",
       "         [-0.2428, -0.0361,  0.2480,  ..., -0.3457, -0.1876,  0.1968],\n",
       "         [-0.2731, -0.0623,  0.1701,  ..., -0.2901, -0.1646,  0.1825]],\n",
       "\n",
       "        [[-0.6753,  0.1509,  0.6095,  ..., -0.1906,  0.1266,  0.2335],\n",
       "         [ 0.3355, -0.1948,  0.7136,  ..., -0.5448, -0.3137,  0.3606],\n",
       "         [-0.3835, -0.1558,  0.2998,  ..., -0.1311,  0.0276,  0.1025],\n",
       "         ...,\n",
       "         [-0.4226, -0.1553,  0.4122,  ..., -0.0666, -0.0163,  0.1318],\n",
       "         [-0.3648, -0.0983,  0.3275,  ..., -0.0584, -0.0056,  0.1314],\n",
       "         [-0.4173, -0.0875,  0.3572,  ..., -0.0654,  0.0922,  0.1060]],\n",
       "\n",
       "        [[-0.2468, -0.1283,  0.7574,  ..., -0.4409,  0.0471,  0.2729],\n",
       "         [-0.5766, -0.1669,  0.1708,  ..., -0.3447, -0.0484,  0.3114],\n",
       "         [-0.6120, -0.2221,  0.1426,  ..., -0.3430, -0.0638,  0.3071],\n",
       "         ...,\n",
       "         [-0.5688, -0.0892,  0.3984,  ..., -0.3302,  0.1014,  0.2614],\n",
       "         [-0.6474,  0.1497,  0.7590,  ..., -0.1187, -0.4105,  0.3743],\n",
       "         [ 0.2170,  0.0641,  0.9122,  ..., -0.2507, -0.2159,  0.2898]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(datamodule)\n",
    "datamodule.sequence_length\n",
    "datamodule.min_sequence_length\n",
    "datamodule.max_sequence_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
