{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from eyemind.trainer.loops import KFoldLoop\n",
    "import eyemind\n",
    "from eyemind.models.transformers import InformerEncoderDecoderModel, InformerEncoderFixationModel, InformerMultiTaskEncoderDecoder\n",
    "from eyemind.dataloading.informer_data import InformerDataModule\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Simply load in already-trained models and generate predictions\n",
    "\n",
    "This shouldn't be this difficult!\n",
    "\n",
    "# Notes\n",
    "\n",
    "\n",
    "in the original informer code, there is a command line option\n",
    "`parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')`\n",
    "Check what this does\n",
    "\n",
    "## encoder only\n",
    "I have managed to get logits from the encoder, but model(batch) does not work when mdoel is the entire multitask enc-decoder stack.\n",
    "\n",
    "## fixation decoder\n",
    "How to load this? \n",
    "A: model.fi_decoder\n",
    "\n",
    "# TODO ideas\n",
    "[x] try loading encoder and fi decoder separately\n",
    "\n",
    "[ ] try modifying train loop to store the preds per batch\n",
    "\n",
    "[ ] within loop, get batch and run thru 2 models - ensure same random subsequence chosen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def fixation_image(pred,target, title=\"Fixation Identification\"):\n",
    "    sl = len(pred)\n",
    "    print(f'sequence length: {sl}')\n",
    "    fixation_labels = torch.cat((pred.expand(sl//2,sl),target.expand(sl//2,sl)))\n",
    "    plt.imshow(fixation_labels, extent=[0, len(fixation_labels[1]),0, 100], cmap='Greys')\n",
    "    # plt.xticks(np.arange(0, len(fixation_labels), 1), [])\n",
    "    plt.yticks([])\n",
    "    # plt.grid(True, axis='x', lw=1, c='black')\n",
    "    # plt.tick_params(axis='x', length=0)\n",
    "    plt.title(title)\n",
    "    black = mpatches.Patch(color='black', label='Fixation')\n",
    "    white = mpatches.Patch(color='white', label='Saccade')\n",
    "    plt.legend(handles=[black, white],bbox_to_anchor=(1.15, 1), loc='upper right')\n",
    "    plt.xlabel(\"Time Steps (100 steps ~ 1.7s)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n",
      "/usr/local/Caskroom/miniconda/base/envs/dg/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "length of batch: 4\n",
      "preds: torch.Size([32, 500, 2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/roso8920/Dropbox (Emotive Computing)/EML Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m fixation_preds \u001b[39m=\u001b[39m preds[\u001b[39m\"\u001b[39;49m\u001b[39mfi\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W2sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m fixation_targets \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m fixation_image(fixation_preds[\u001b[39m1\u001b[39m], fixation_targets[\u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mFixation Identification - Informer\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# Load our trained encoder decoder from checkpoint for each fold\n",
    "# this shouold load the encoder and its 4 decoders\n",
    "\n",
    "# using pytorch lightning module\n",
    "# https://lightning.ai/docs/pytorch/stable/deploy/production_basic.html\n",
    "\n",
    "repodir = os.path.dirname(os.path.dirname(eyemind.__file__))\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    save_dir = f\"{repodir}/lightning_logs/informer_pretraining_seed21/fold{fold}/\"\n",
    "    config_path=os.path.join(save_dir,\"config.yaml\")\n",
    "    ckpt_path = os.path.join(save_dir,\"checkpoints\",\"last.ckpt\")\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    seed_everything(config[\"seed_everything\"], workers=True) # not sure if this is needed\n",
    "\n",
    "    model = InformerMultiTaskEncoderDecoder.load_from_checkpoint(ckpt_path,\n",
    "                                                    # encoder_weights_path=None\n",
    "                                                    )\n",
    "    encoder=model.encoder\n",
    "    decoder=model.fi_decoder\n",
    "    model.eval()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # set up an InformerDataModule to load the same data as used in training\n",
    "    # trainer = Trainer(**config[\"trainer\"])\n",
    "    data_dir = os.path.join(repodir,config[\"data\"][\"data_dir\"])\n",
    "    label_file = os.path.join(repodir,config[\"data\"][\"label_filepath\"])\n",
    "    config[\"data\"][\"data_dir\"]=data_dir\n",
    "    config[\"data\"][\"label_filepath\"]=label_file\n",
    "    \n",
    "    datamodule = InformerDataModule(**config[\"data\"])\n",
    "    datamodule.setup()\n",
    "\n",
    "    test_dl = datamodule.get_dataloader(datamodule.test_dataset) # this is the held out fold's dataloader\n",
    "    for i,batch in enumerate(test_dl):\n",
    "        print(f\"batch: {i}\")\n",
    "        print(f\"length of batch: {len(batch)}\")\n",
    "        with torch.no_grad():\n",
    "            # preds = model(batch) # this step fails\n",
    "            logits=encoder(batch[0], None)\n",
    "            fixation_logits = model.fi_decoder.forward(logits)\n",
    "            fixation_preds=fixation_logits.max(2).indices\n",
    "            fixation_targets = batch[1]\n",
    "\n",
    "        if i==4: # just run a couple to check\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([32, 500, 2])\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 2])\n",
      "torch.Size([32])\n",
      "removing labels changed the predicitons\n",
      "logits shape: torch.Size([32, 500, 512])\n"
     ]
    }
   ],
   "source": [
    "# Try running encoder on batch\n",
    "# batch size is 32\n",
    "# but batch contains 4 elements each of length 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((len(batch)))\n",
    "    print(batch[0].shape) # <- sequence\n",
    "    print(batch[1].shape) # <- fixation targets, but we want to pretend we don't know these! is that \"x_dec\" ?\n",
    "    # print(batch[1][0:10]) # Contains 0 and 1\n",
    "    print(batch[2].shape) # <- second sequence for contrastive??\n",
    "    print(batch[3].shape) # <- Some binary label per instance\n",
    "\n",
    "    logits2 = encoder.forward(batch[0], batch[1]) # this works, but batch[1] contains the labels when we are trying to predict them.\n",
    "    logits = encoder.forward(batch[0], None) # this works, setting the labels to None. Hooray! \n",
    "    if torch.equal(logits2, logits):\n",
    "        print('providing labels made no difference')\n",
    "    else:\n",
    "        print('removing labels changed the predicitons')\n",
    "\n",
    "    print(f'logits shape: {logits.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can see the attributes of the full model using dir\n",
    "# dir(model)\n",
    "# # it has a module list of decoders\n",
    "# dir(model.decoders)\n",
    "\n",
    "# Apart from pc we can access each task's decoder like so:\n",
    "print(f'FI decoder: {model.fi_decoder}')\n",
    "print(f'RC decoder: {model.rc_decoder}')\n",
    "print(f'CL decoder: {model.cl_decoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FI logits: torch.Size([32, 500, 2])\n",
      "tensor([[1, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 1, 1,  ..., 0, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 1, 0, 0],\n",
      "        [0, 1, 1,  ..., 1, 0, 1],\n",
      "        [0, 1, 1,  ..., 1, 1, 0]])\n",
      "torch.Size([500])\n",
      "torch.Size([500])\n",
      "sequence length: 500\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Great! Let's use the logits from the encoder as input to the fi decoder...\n",
    "fixation_logits = model.fi_decoder.forward(logits)\n",
    "print(f\"FI logits: {fixation_logits.shape}\")\n",
    "# this is of shape n, len, 2. I assume the 2 dimensions here are prob(fix) and prob(sacc)? \n",
    "# do we need to just take one dimension? And softmax then threshold >/5?\n",
    "fixation_preds=fixation_logits.max(2).indices\n",
    "print(fixation_preds)\n",
    "fixation_targets = batch[1]\n",
    "\n",
    "# or reshape batch into one long vector in Ricks's code, to get batch-wisemetric:\n",
    "logits_long = fixation_logits.squeeze().reshape(-1,2)\n",
    "targets_long = fixation_targets.reshape(-1).long()\n",
    "\n",
    "# pick one from batch to plot\n",
    "one_pred = fixation_preds[0,:]\n",
    "one_target = fixation_targets[0,:]\n",
    "print(one_pred.shape)\n",
    "print(one_target.shape)\n",
    "\n",
    "fixation_image(one_pred, one_target, \"Fixation Identification - Informer\")\n",
    "\n",
    "# SUCCESS!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchmetrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/roso8920/Dropbox (Emotive Computing)/EML Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# evaluate FI \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfi_metric \u001b[39m=\u001b[39m torchmetrics\u001b[39m.\u001b[39mAUROC(num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m targets \u001b[39m=\u001b[39m fix_y\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mlong()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchmetrics' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate FI \n",
    "\n",
    "#mask = torch.any(X == -180, dim=1)\n",
    "loss = self.fi_criterion(logits_long, targets_long)\n",
    "preds = self._get_preds(logits)\n",
    "probs = self._get_probs(logits)\n",
    "targets = targets.int()\n",
    "auprc = model.fi_metric(probs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InformerMultiTaskEncoderDecoder._get_preds of InformerMultiTaskEncoderDecoder(\n",
       "  (encoder): InformerEncoder(\n",
       "    (enc_embedding): GazeEmbedding(\n",
       "      (value_embedding): TokenEmbedding(\n",
       "        (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "      )\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (attn_layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fi_decoder): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (fi_criterion): CrossEntropyLoss()\n",
       "  (fi_metric): AveragePrecision()\n",
       "  (pc_decoder): InformerDecoder(\n",
       "    (dec_embedding): GazeEmbedding(\n",
       "      (value_embedding): TokenEmbedding(\n",
       "        (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "      )\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (pc_criterion): RMSELoss(\n",
       "    (mse): MSELoss()\n",
       "  )\n",
       "  (pc_metric): MeanSquaredError()\n",
       "  (cl_decoder): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cl_criterion): CrossEntropyLoss()\n",
       "  (cl_metric): Accuracy()\n",
       "  (rc_decoder): InformerDecoder(\n",
       "    (dec_embedding): GazeEmbedding(\n",
       "      (value_embedding): TokenEmbedding(\n",
       "        (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "      )\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (rc_criterion): RMSELoss(\n",
       "    (mse): MSELoss()\n",
       "  )\n",
       "  (rc_metric): MeanSquaredError()\n",
       "  (decoders): ModuleList(\n",
       "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (1): InformerDecoder(\n",
       "      (dec_embedding): GazeEmbedding(\n",
       "        (value_embedding): TokenEmbedding(\n",
       "          (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "        )\n",
       "        (position_embedding): PositionalEmbedding()\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "    (2): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InformerDecoder(\n",
       "      (dec_embedding): GazeEmbedding(\n",
       "        (value_embedding): TokenEmbedding(\n",
       "          (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "        )\n",
       "        (position_embedding): PositionalEmbedding()\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fi_metric\n",
    "model._get_preds # TODO: use this to get predictions from logits\n",
    "model._get_probs # TODO: use this to get probs from logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
