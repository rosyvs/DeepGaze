{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/dg/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from eyemind.trainer.loops import KFoldLoop\n",
    "import eyemind\n",
    "from eyemind.models.transformers import InformerEncoderDecoderModel, InformerEncoderFixationModel, InformerMultiTaskEncoderDecoder\n",
    "from eyemind.dataloading.informer_data import InformerDataModule\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Simply load in already-trained models and generate predictions\n",
    "\n",
    "This shouldn't be this difficult!\n",
    "\n",
    "# Notes\n",
    "\n",
    "\n",
    "in the original informer code, there is a command line option\n",
    "`parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')`\n",
    "Check what this does\n",
    "\n",
    "## encoder only\n",
    "I have managed to get logits from the encoder, but model(batch) does not work when mdoel is the entire multitask enc-decoder stack.\n",
    "\n",
    "## fixation decoder\n",
    "How to load this? \n",
    "A: model.fi_decoder\n",
    "\n",
    "# TODO ideas\n",
    "[x] try loading encoder and fi decoder separately\n",
    "\n",
    "[ ] get predictions for other tasks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def fixation_image(pred,target, title=\"Fixation Identification\"):\n",
    "    sl = len(pred)\n",
    "    print(f'sequence length: {sl}')\n",
    "    fixation_labels = torch.cat((pred.expand(sl//2,sl),target.expand(sl//2,sl)))\n",
    "    plt.imshow(fi_labels, extent=[0, len(fi_labels[1]),0, 100], cmap='Greys')\n",
    "    # plt.xticks(np.arange(0, len(fixation_labels), 1), [])\n",
    "    plt.yticks([])\n",
    "    # plt.grid(True, axis='x', lw=1, c='black')\n",
    "    # plt.tick_params(axis='x', length=0)\n",
    "    plt.title(title)\n",
    "    black = mpatches.Patch(color='black', label='Fixation')\n",
    "    white = mpatches.Patch(color='white', label='Saccade')\n",
    "    plt.legend(handles=[black, white],bbox_to_anchor=(1.15, 1), loc='upper right')\n",
    "    plt.xlabel(\"Time Steps (100 steps ~ 1.7s)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/dg/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "length of batch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n",
      "/usr/local/Caskroom/miniconda/base/envs/dg/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "length of batch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n",
      "/usr/local/Caskroom/miniconda/base/envs/dg/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "length of batch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n",
      "/usr/local/Caskroom/miniconda/base/envs/dg/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "length of batch: 4\n"
     ]
    }
   ],
   "source": [
    "# Load our trained encoder decoder from checkpoint for each fold\n",
    "# this shouold load the encoder and its 4 decoders\n",
    "\n",
    "# using pytorch lightning module\n",
    "# https://lightning.ai/docs/pytorch/stable/deploy/production_basic.html\n",
    "\n",
    "repodir = os.path.dirname(os.path.dirname(eyemind.__file__))\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    save_dir = f\"{repodir}/lightning_logs/informer_pretraining_seed21/fold{fold}/\"\n",
    "    config_path=os.path.join(save_dir,\"config.yaml\")\n",
    "    ckpt_path = os.path.join(save_dir,\"checkpoints\",\"last.ckpt\")\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    seed_everything(config[\"seed_everything\"], workers=True) # not sure if this is needed\n",
    "\n",
    "    model = InformerMultiTaskEncoderDecoder.load_from_checkpoint(ckpt_path,\n",
    "                                                    # encoder_weights_path=None\n",
    "                                                    )\n",
    "    encoder=model.encoder\n",
    "    decoder=model.fi_decoder\n",
    "    model.eval()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # set up an InformerDataModule to load the same data as used in training\n",
    "    # trainer = Trainer(**config[\"trainer\"])\n",
    "    data_dir = os.path.join(repodir,config[\"data\"][\"data_dir\"])\n",
    "    label_file = os.path.join(repodir,config[\"data\"][\"label_filepath\"])\n",
    "    config[\"data\"][\"data_dir\"]=data_dir\n",
    "    config[\"data\"][\"label_filepath\"]=label_file\n",
    "    \n",
    "    datamodule = InformerDataModule(**config[\"data\"])\n",
    "    datamodule.setup()\n",
    "\n",
    "    test_dl = datamodule.get_dataloader(datamodule.test_dataset) # this is the held out fold's dataloader\n",
    "    for i,batch in enumerate(test_dl):\n",
    "        print(f\"batch: {i}\")\n",
    "        print(f\"length of batch: {len(batch)}\")\n",
    "        with torch.no_grad():\n",
    "            # preds = model(batch) # this step fails\n",
    "            logits=encoder(batch[0], None)\n",
    "            fixation_logits = model.fi_decoder.forward(logits)\n",
    "            fixation_preds=fixation_logits.max(2).indices\n",
    "            fixation_targets = batch[1]\n",
    "\n",
    "        if i==0: # just run a couple to check\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([32, 500, 2])\n",
      "torch.Size([32, 500])\n",
      "torch.Size([32, 500, 2])\n",
      "torch.Size([32])\n",
      "removing labels changed the predicitons\n",
      "logits shape: torch.Size([32, 500, 512])\n"
     ]
    }
   ],
   "source": [
    "# Try running encoder on batch\n",
    "# batch size is 32\n",
    "# but batch contains 4 elements each of length 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((len(batch)))\n",
    "    print(batch[0].shape) # <- sequence\n",
    "    print(batch[1].shape) # <- fixation targets, but we want to pretend we don't know these! is that \"x_dec\" ?\n",
    "    # print(batch[1][0:10]) # Contains 0 and 1\n",
    "    print(batch[2].shape) # <- second sequence for contrastive??\n",
    "    print(batch[3].shape) # <- Some binary label per instance\n",
    "\n",
    "    logits2 = encoder.forward(batch[0], batch[1]) # this works, but batch[1] contains the labels when we are trying to predict them.\n",
    "    logits = encoder.forward(batch[0], None) # this works, setting the labels to None. Hooray! \n",
    "    if torch.equal(logits2, logits):\n",
    "        print('providing labels made no difference')\n",
    "    else:\n",
    "        print('removing labels changed the predicitons')\n",
    "\n",
    "    print(f'logits shape: {logits.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can see the attributes of the full model using dir\n",
    "# dir(model)\n",
    "# # it has a module list of decoders\n",
    "# dir(model.decoders)\n",
    "\n",
    "# Apart from pc we can access each task's decoder like so:\n",
    "print(f'FI decoder: {model.fi_decoder}')\n",
    "print(f'RC decoder: {model.rc_decoder}')\n",
    "print(f'CL decoder: {model.cl_decoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/roso8920/Dropbox (Emotive Computing)/EML Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Great! Let's use the logits from the encoder as input to the fi decoder...\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m fixation_logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfi_decoder\u001b[39m.\u001b[39mforward(logits)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFI logits: \u001b[39m\u001b[39m{\u001b[39;00mfixation_logits\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# this is of shape n, len, 2. I assume the 2 dimensions here are prob(fix) and prob(sacc)? \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# do we need to just take one dimension? And softmax then threshold >/5?\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Great! Let's use the logits from the encoder as input to the fi decoder...\n",
    "fixation_logits = model.fi_decoder.forward(logits)\n",
    "print(f\"FI logits: {fixation_logits.shape}\")\n",
    "# this is of shape n, len, 2. I assume the 2 dimensions here are prob(fix) and prob(sacc)? \n",
    "# do we need to just take one dimension? And softmax then threshold >/5?\n",
    "fixation_preds=fixation_logits.max(2).indices\n",
    "print(fixation_preds)\n",
    "fixation_targets = batch[1]\n",
    "\n",
    "# or reshape batch into one long vector in Ricks's code, to get batch-wisemetric:\n",
    "logits_long = fixation_logits.squeeze().reshape(-1,2)\n",
    "targets_long = fixation_targets.reshape(-1).long()\n",
    "\n",
    "# pick one from batch to plot\n",
    "one_pred = fixation_preds[0,:]\n",
    "one_target = fixation_targets[0,:]\n",
    "print(one_pred.shape)\n",
    "print(one_target.shape)\n",
    "\n",
    "fixation_image(one_pred, one_target, \"Fixation Identification - Informer\")\n",
    "\n",
    "# SUCCESS!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/roso8920/Dropbox (Emotive Computing)/EML Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# evaluate FI \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#mask = torch.any(X == -180, dim=1)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfi_criterion(logits_long, targets_long)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_get_preds(logits)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/roso8920/Dropbox%20%28Emotive%20Computing%29/EML%20Rosy/DeepGaze/notebooks/get_predictions_for_stats.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m probs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_get_probs(logits)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate FI \n",
    "\n",
    "#mask = torch.any(X == -180, dim=1)\n",
    "loss = model.fi_criterion(logits_long, targets_long)\n",
    "preds = model._get_preds(logits)\n",
    "probs = model._get_probs(logits)\n",
    "targets = targets.int()\n",
    "auprc = model.fi_metric(probs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InformerMultiTaskEncoderDecoder._get_preds of InformerMultiTaskEncoderDecoder(\n",
       "  (encoder): InformerEncoder(\n",
       "    (enc_embedding): GazeEmbedding(\n",
       "      (value_embedding): TokenEmbedding(\n",
       "        (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "      )\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (attn_layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fi_decoder): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (fi_criterion): CrossEntropyLoss()\n",
       "  (fi_metric): AveragePrecision()\n",
       "  (pc_decoder): InformerDecoder(\n",
       "    (dec_embedding): GazeEmbedding(\n",
       "      (value_embedding): TokenEmbedding(\n",
       "        (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "      )\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (pc_criterion): RMSELoss(\n",
       "    (mse): MSELoss()\n",
       "  )\n",
       "  (pc_metric): MeanSquaredError()\n",
       "  (cl_decoder): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cl_criterion): CrossEntropyLoss()\n",
       "  (cl_metric): Accuracy()\n",
       "  (rc_decoder): InformerDecoder(\n",
       "    (dec_embedding): GazeEmbedding(\n",
       "      (value_embedding): TokenEmbedding(\n",
       "        (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "      )\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attention): AttentionLayer(\n",
       "            (inner_attention): ProbAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (rc_criterion): RMSELoss(\n",
       "    (mse): MSELoss()\n",
       "  )\n",
       "  (rc_metric): MeanSquaredError()\n",
       "  (decoders): ModuleList(\n",
       "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (1): InformerDecoder(\n",
       "      (dec_embedding): GazeEmbedding(\n",
       "        (value_embedding): TokenEmbedding(\n",
       "          (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "        )\n",
       "        (position_embedding): PositionalEmbedding()\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "    (2): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InformerDecoder(\n",
       "      (dec_embedding): GazeEmbedding(\n",
       "        (value_embedding): TokenEmbedding(\n",
       "          (tokenConv): Conv1d(2, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "        )\n",
       "        (position_embedding): PositionalEmbedding()\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (cross_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (projection): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fi_metric\n",
    "model._get_preds # TODO: use this to get predictions from logits\n",
    "model._get_probs # TODO: use this to get probs from logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed_everything': 21,\n",
       " 'trainer': {'logger': {'class_path': 'pytorch_lightning.loggers.TensorBoardLogger',\n",
       "   'init_args': {'save_dir': './lightning_logs',\n",
       "    'name': 'informer_pretraining_seed21',\n",
       "    'version': 'fold3',\n",
       "    'log_graph': False,\n",
       "    'default_hp_metric': True,\n",
       "    'prefix': '',\n",
       "    'sub_dir': None,\n",
       "    'agg_key_funcs': None,\n",
       "    'agg_default_func': None,\n",
       "    'comment': '',\n",
       "    'purge_step': None,\n",
       "    'max_queue': 10,\n",
       "    'flush_secs': 120,\n",
       "    'filename_suffix': ''}},\n",
       "  'enable_checkpointing': True,\n",
       "  'callbacks': [{'class_path': 'pytorch_lightning.callbacks.ModelCheckpoint',\n",
       "    'init_args': {'dirpath': None,\n",
       "     'filename': None,\n",
       "     'monitor': 'val_loss',\n",
       "     'verbose': False,\n",
       "     'save_last': True,\n",
       "     'save_top_k': 1,\n",
       "     'save_weights_only': False,\n",
       "     'mode': 'min',\n",
       "     'auto_insert_metric_name': True,\n",
       "     'every_n_train_steps': None,\n",
       "     'train_time_interval': None,\n",
       "     'every_n_epochs': None,\n",
       "     'save_on_train_epoch_end': None}}],\n",
       "  'default_root_dir': None,\n",
       "  'gradient_clip_val': 0.5,\n",
       "  'gradient_clip_algorithm': None,\n",
       "  'num_nodes': 1,\n",
       "  'num_processes': None,\n",
       "  'devices': None,\n",
       "  'gpus': None,\n",
       "  'auto_select_gpus': False,\n",
       "  'tpu_cores': None,\n",
       "  'ipus': None,\n",
       "  'enable_progress_bar': True,\n",
       "  'overfit_batches': 0.0,\n",
       "  'track_grad_norm': -1,\n",
       "  'check_val_every_n_epoch': 1,\n",
       "  'fast_dev_run': False,\n",
       "  'accumulate_grad_batches': None,\n",
       "  'max_epochs': 200,\n",
       "  'min_epochs': None,\n",
       "  'max_steps': -1,\n",
       "  'min_steps': None,\n",
       "  'max_time': None,\n",
       "  'limit_train_batches': None,\n",
       "  'limit_val_batches': None,\n",
       "  'limit_test_batches': None,\n",
       "  'limit_predict_batches': None,\n",
       "  'val_check_interval': None,\n",
       "  'log_every_n_steps': 50,\n",
       "  'accelerator': 'gpu',\n",
       "  'strategy': None,\n",
       "  'sync_batchnorm': False,\n",
       "  'precision': 32,\n",
       "  'enable_model_summary': True,\n",
       "  'weights_save_path': None,\n",
       "  'num_sanity_val_steps': 2,\n",
       "  'resume_from_checkpoint': 'lightning_logs/informer_pretraining_seed21/fold3/checkpoints/epoch=181-step=9282.ckpt',\n",
       "  'profiler': None,\n",
       "  'benchmark': None,\n",
       "  'deterministic': None,\n",
       "  'reload_dataloaders_every_n_epochs': 0,\n",
       "  'auto_lr_find': False,\n",
       "  'replace_sampler_ddp': True,\n",
       "  'detect_anomaly': False,\n",
       "  'auto_scale_batch_size': 'power',\n",
       "  'plugins': None,\n",
       "  'amp_backend': 'native',\n",
       "  'amp_level': None,\n",
       "  'move_metrics_to_cpu': False,\n",
       "  'multiple_trainloader_mode': 'max_size_cycle'},\n",
       " 'model': {'tasks': ['fi', 'pc', 'cl', 'rc'],\n",
       "  'enc_in': 2,\n",
       "  'dec_in': 2,\n",
       "  'c_out': 2,\n",
       "  'seq_len': 500,\n",
       "  'label_len': 100,\n",
       "  'pred_len': 150,\n",
       "  'padding': 0,\n",
       "  'factor': 5,\n",
       "  'd_model': 512,\n",
       "  'n_heads': 8,\n",
       "  'e_layers': 3,\n",
       "  'd_layers': 2,\n",
       "  'd_ff': 512,\n",
       "  'dropout': 0.05,\n",
       "  'attn': 'prob',\n",
       "  'activation': 'gelu',\n",
       "  'output_attention': False,\n",
       "  'distil': False,\n",
       "  'mix': True,\n",
       "  'class_weights': [2.43, 0.63],\n",
       "  'learning_rate': 0.001,\n",
       "  'freeze_encoder': False},\n",
       " 'data': {'data_dir': '/Users/roso8920/Dropbox (Emotive Computing)/EML Rosy/DeepGaze/./data/processed/fixation',\n",
       "  'label_filepath': '/Users/roso8920/Dropbox (Emotive Computing)/EML Rosy/DeepGaze/./data/processed/EML1_pageLevel_500+_matchEDMinstances.csv',\n",
       "  'load_setup_path': None,\n",
       "  'test_dir': None,\n",
       "  'train_dataset': None,\n",
       "  'val_dataset': None,\n",
       "  'test_dataset': None,\n",
       "  'train_fold': None,\n",
       "  'val_fold': None,\n",
       "  'sequence_length': 500,\n",
       "  'label_length': 48,\n",
       "  'pred_length': None,\n",
       "  'num_workers': 4,\n",
       "  'batch_size': 32,\n",
       "  'pin_memory': True,\n",
       "  'drop_last': True,\n",
       "  'min_scanpath_length': 500,\n",
       "  'contrastive': True},\n",
       " 'fold_number': 3,\n",
       " 'split_filepath': './data_splits/4fold_participant/seed21.yml',\n",
       " 'num_folds': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
